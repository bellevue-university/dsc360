{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 4 - Using Natural Language Processing\n",
    "This week follows chapter 3 from the text.\n",
    "\n",
    "## Page 118"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'a name=\"generator\" content=\"Ebookmaker 0.10.0 by Project Gutenberg\"/>\\r\\n</head>\\r\\n  <body><p id=\"id00000\">Project Gutenberg EBook The Bible, King James, Book 1: Genesis</p>\\r\\n\\r\\n<p id=\"id00001\">Copyright laws are changing all over the world. Be sure to check the\\r\\ncopyright laws for your country before downloading or redistributing\\r\\nthis or any other Project Gutenberg eBook.</p>\\r\\n\\r\\n<p id=\"id00002\">This header should be the first thing seen when viewing this Project\\r\\nGutenberg file.  Please do not remove it.  Do not change or edit the\\r\\nheader without written permission.</p>\\r\\n\\r\\n<p id=\"id00003\">Please read the \"legal small print,\" and other information about the\\r\\neBook and Project Gutenberg at the bottom of this file.  Included is\\r\\nimportant information about your specific rights and restrictions in\\r\\nhow the file may be used.  You can also find out about how to make a\\r\\ndonation to Project Gutenberg, and how to get involved.</p>\\r\\n\\r\\n<p id=\"id00004\" style=\"margin-top: 2em\">**Welcome To The World of Free Plain Vanilla Electronic Text' \n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.content\n",
    "# the text that prints is a little different because of book version differences\n",
    "print(content[1163:2200], '\\n')"
   ]
  },
  {
   "source": [
    "## Pages 118-119"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\nThis eBook was produced by David Widger\nwith the help of Derek Andrew's text from January 1992\nand the work of Bryan Taylor in November 2002.\nBook 01        Genesis\n01:001:001 In the beginning God created the heaven and the earth.\n01:001:002 And the earth was without form, and void; and darkness was\n           upon the face of the deep. And the Spirit of God moved upon\n           the face of the waters.\n01:001:003 And God said, Let there be light: and there was light.\n01:001:004 And God saw the light, that it was good: and God divided the\n           light from the darkness.\n01:001:005 And God called the light Day, and the darkness he called\n           Night. And the evening and the morning were the first day.\n01:001:006 And God said, Let there be a firmament in the midst of the\n           waters, \n\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:2045], '\\n')"
   ]
  },
  {
   "source": [
    "## Tokenizer\n",
    "Shorter text than the text, but this is the same core code, which comes after showing some data about the Alice corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample text:  We will discuss briefly about the basic syntax, structure and design philosophies.  There is a defined hierarchical syntax for Python code which you should remember  when writing code! Python is a really powerful programming language! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "## SENTENCE TOKENIZATION\n",
    "# loading text corpora\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "\n",
    "sample_text = 'We will discuss briefly about the basic syntax,\\\n",
    " structure and design philosophies. \\\n",
    " There is a defined hierarchical syntax for Python code which you should remember \\\n",
    " when writing code! Python is a really powerful programming language!'\n",
    "print('Sample text: ', sample_text, '\\n')"
   ]
  },
  {
   "source": [
    "### Output a bit of Alice in Wonderland"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of alice:  144395\n",
      "First 100 chars of alice:  [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total characters in Alice in Wonderland\n",
    "print('Length of alice: ', len(alice))\n",
    "# First 100 characters in the corpus\n",
    "print('First 100 chars of alice: ', alice[0:100], '\\n')"
   ]
  },
  {
   "source": [
    "\n",
    "## Default Sentence Tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Default sentence tokenizer\n",
      "Total sentences in sample_text: 3\n",
      "Sample text sentences :-\n",
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember  when writing code!',\n",
      " 'Python is a really powerful programming language!']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Default sentence tokenizer')\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "pprint(sample_sentences)\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:-')\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "source": [
    "## Other Languages Sentence Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Other language tokenization\n",
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n",
      "German tokenizer type: <class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n",
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('europarl_raw')\n",
    "from nltk.corpus import europarl_raw\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "print('Other language tokenization')\n",
    "# Total characters in the corpus\n",
    "print(len(german_text))\n",
    "# First 100 characters in the corpus\n",
    "print(german_text[0:100])\n",
    "\n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "# verify the type of german_tokenizer\n",
    "# should be PunktSentenceTokenizer\n",
    "print('German tokenizer type:', type(german_tokenizer))\n",
    "\n",
    "# check if results of both tokenizers match\n",
    "# should be True\n",
    "print(german_sentences_def == german_sentences)\n",
    "# print(first 5 sentences of the corpus\n",
    "for sent in german_sentences[0:5]:\n",
    "    print(sent)\n",
    "print('\\n')"
   ]
  },
  {
   "source": [
    "## Using Punkt Tokenizer for Sentence Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Punkt tokenizer\n",
      "array(['We will discuss briefly about the basic syntax, structure and design philosophies.',\n",
      "       'There is a defined hierarchical syntax for Python code which you should remember  when writing code!',\n",
      "       'Python is a really powerful programming language!'], dtype='<U100')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Punkt tokenizer')\n",
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(np.array(sample_sentences))\n",
    "print('\\n')"
   ]
  },
  {
   "source": [
    "\n",
    "## Using RegexpTokenizer for Sentence Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Regex tokenizer\n",
      "['We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " ' There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember  when writing code!',\n",
      " 'Python is a really powerful programming language!']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Regex tokenizer')\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "            pattern=SENTENCE_TOKENS_PATTERN,\n",
    "            gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "# again, the output is different because the sample sentence is different\n",
    "pprint(sample_sentences)\n",
    "print('\\n')"
   ]
  },
  {
   "source": [
    "## Work Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word tokenizer\n",
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "# default word tokenizer\n",
    "print('Word tokenizer')\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print(words, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Treebank tokenizer\n",
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race'] \n",
      "\n",
      "TokTok tokenizer\n",
      "['We' 'will' 'discuss' 'briefly' 'about' 'the' 'basic' 'syntax' ','\n",
      " 'structure' 'and' 'design' 'philosophies.' 'There' 'is' 'a' 'defined'\n",
      " 'hierarchical' 'syntax' 'for' 'Python' 'code' 'which' 'you' 'should'\n",
      " 'remember' 'when' 'writing' 'code' '!' 'Python' 'is' 'a' 'really'\n",
      " 'powerful' 'programming' 'language' '!'] \n",
      "\n",
      "RegEx word tokenizer\n",
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n",
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race'] \n",
      "\n",
      "Derived RegEx tokenizers\n",
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race'] \n",
      "\n",
      "Whitespace Tokenizer\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Treebank tokenizer')\n",
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words, '\\n')\n",
    "\n",
    "# toktok tokenizer\n",
    "print('TokTok tokenizer')\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "words = tokenizer.tokenize(sample_text)\n",
    "print(np.array(words), '\\n')\n",
    "\n",
    "# regex word tokenizer\n",
    "print('RegEx word tokenizer')\n",
    "TOKEN_PATTERN = r'\\w+'        \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,\n",
    "                                gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)\n",
    "\n",
    "GAP_PATTERN = r'\\s+'        \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
    "                                gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)\n",
    "\n",
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print(word_indices)\n",
    "print([sentence[start:end] for start, end in word_indices], '\\n')\n",
    "\n",
    "# derived regex tokenizers\n",
    "print(\"Derived RegEx tokenizers\")\n",
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words, '\\n')\n",
    "\n",
    "# whitespace tokenizer\n",
    "print('Whitespace Tokenizer')\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words, '\\n')"
   ]
  },
  {
   "source": [
    "## Pages 132 - 134"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Robust tokenizer - NLTK\n",
      "[list(['We', 'will', 'discuss', 'briefly', 'about', 'the', 'basic', 'syntax', ',', 'structure', 'and', 'design', 'philosophies', '.'])\n",
      " list(['There', 'is', 'a', 'defined', 'hierarchical', 'syntax', 'for', 'Python', 'code', 'which', 'you', 'should', 'remember', 'when', 'writing', 'code', '!'])\n",
      " list(['Python', 'is', 'a', 'really', 'powerful', 'programming', 'language', '!'])] \n",
      "\n",
      "['We' 'will' 'discuss' 'briefly' 'about' 'the' 'basic' 'syntax' ','\n",
      " 'structure' 'and' 'design' 'philosophies' '.' 'There' 'is' 'a' 'defined'\n",
      " 'hierarchical' 'syntax' 'for' 'Python' 'code' 'which' 'you' 'should'\n",
      " 'remember' 'when' 'writing' 'code' '!' 'Python' 'is' 'a' 'really'\n",
      " 'powerful' 'programming' 'language' '!'] \n",
      "\n",
      "spaCy...\n",
      "[We will discuss briefly about the basic syntax, structure and design philosophies.\n",
      " There is a defined hierarchical syntax for Python code which you should remember  when writing code!\n",
      " Python is a really powerful programming language!] \n",
      "\n",
      "[list(['We', 'will', 'discuss', 'briefly', 'about', 'the', 'basic', 'syntax', ',', 'structure', 'and', 'design', 'philosophies', '.'])\n",
      " list(['There', 'is', 'a', 'defined', 'hierarchical', 'syntax', 'for', 'Python', 'code', 'which', 'you', 'should', 'remember', 'when', 'writing', 'code', '!'])\n",
      " list(['Python', 'is', 'a', 'really', 'powerful', 'programming', 'language', '!'])] \n",
      "\n",
      "[We will discuss briefly about the basic syntax , structure and design\n",
      " philosophies .   There is a defined hierarchical syntax for Python code\n",
      " which you should remember   when writing code ! Python is a really\n",
      " powerful programming language !] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Robust tokenizer - NLTK')\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "sents = tokenize_text(sample_text)\n",
    "print(np.array(sents),'\\n')\n",
    "\n",
    "words = [word for sentence in sents for word in sentence]\n",
    "print(np.array(words), '\\n')\n",
    "\n",
    "print('spaCy...')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "text_spacy = nlp(sample_text)\n",
    "print(np.array(list(text_spacy.sents)), '\\n')\n",
    "\n",
    "sent_words = [[word for word in sent] for sent in sents]\n",
    "print(np.array(sent_words), '\\n')\n",
    "\n",
    "# in spacy documentation, this is usually written as [token for token in doc]\n",
    "words = [word for word in text_spacy]\n",
    "print(np.array(words), '\\n')"
   ]
  },
  {
   "source": [
    "## Page 135"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Some Accented cliche facades\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii',\n",
    "                                                      'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "print(remove_accented_chars('Sòme Åccentềd cliché façades'))"
   ]
  },
  {
   "source": [
    "# Expanding Contractions\n",
    "Starting on page 136"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exanding contractions:\n",
      "You all cannot expand contractions I would think \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expaned_text = contractions_pattern.sub(expand_match, sentence)\n",
    "    expanded_text = re.sub(\"'\", \"\", expaned_text)\n",
    "    return expanded_text\n",
    "print('Exanding contractions:')\n",
    "print(expand_contractions(\"Y'all can't expand contractions I'd think\"), '\\n')"
   ]
  },
  {
   "source": [
    "# Removing special characters, page 138"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Remove special characters:\n",
      "Well this was fun What do you think  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits =False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "print('Remove special characters:')\n",
    "print(remove_special_characters('Well this was fun! What do you think? 123#@!', remove_digits=True), '\\n')"
   ]
  },
  {
   "source": [
    "## Case Conversion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Case conversions:\n",
      "the quick brown fox jumped over the big dog\n",
      "THE QUICK BROWN FOX JUMPED OVER THE BIG DOG\n",
      "The Quick Brown Fox Jumped Over The Big Dog \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Case conversions:')\n",
    "# lowercase\n",
    "text = 'The quick brown fox jumped over The Big Dog'\n",
    "print(text.lower())\n",
    "# uppercase\n",
    "print(text.upper())\n",
    "# title case\n",
    "print(text.title(), '\\n')"
   ]
  },
  {
   "source": [
    "## Correcting repeating characters - pages 139-140"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Step: 4 Word: finaly\n",
      "Final word:  finaly \n",
      "\n"
     ]
    }
   ],
   "source": [
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    # remove on repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    if new_word != old_word:\n",
    "        print('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1 #update step\n",
    "        # update old word to last substituted state\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print('Final word: ', new_word, '\\n')\n",
    "        break"
   ]
  },
  {
   "source": [
    "## Pages 140-141 - Wordnet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wordnet:\n",
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Final correct word:  finally \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Wordnet:')\n",
    "from nltk.corpus import wordnet\n",
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    # check for semantically correct words\n",
    "    if wordnet.synsets(old_word):\n",
    "        print('Final correct word: ', old_word, '\\n')\n",
    "        break\n",
    "    # remove on repeated characters\n",
    "    new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "    if new_word != old_word:\n",
    "        print('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1  # update step\n",
    "        # update old word to last substituted state\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print('Final word: ', new_word, '\\n')\n",
    "        break"
   ]
  },
  {
   "source": [
    "## Pages 141-142 - Remove repeated characters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My school is really amazing \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
    "print(' '.join(correct_tokens), '\\n')"
   ]
  },
  {
   "source": [
    "## Spelling Corrector Part 1 - Starting on Page 143"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "qvly', 'foianllx', 'fienllyt', 'fianslx', 'dianllg', 'finnlqly', 'afuanlly', 'fianlaxy', 'fiaxllty', 'ftqianlly', 'fiaullyq', 'fivnllxy', 'fsiantlly', 'fianlayz', 'fianlipy', 'finallt', 'fiaklyy', 'foignlly', 'firanlcly', 'fiaymly', 'fyianlzly', 'fiaqnlfly', 'fjiainlly', 'wfdianlly', 'franoly', 'fianfls', 'fiapljy', 'uiynlly', 'fianxfy', 'ufianlfly', 'fiaswly', 'ofiafnlly', 'fianlswly', 'fianevy', 'fiianllwy', 'wfianlfly', 'fiawllly', 'fiaqllty', 'kfianlfly', 'fsanlry', 'fianlzlyj', 'fienlwly', 'fiuanlhly', 'kfiankly', 'fianlawly', 'fiajjnlly', 'rqfianlly', 'lianllsy', 'fcavnlly', 'fkiahlly', 'fianlfyd', 'efiacnlly', 'fidabnlly', 'fianlxlny', 'fnablly', 'fianllyms', 'fianloc', 'fiaoljy', 'fxnianlly', 'bfiaully', 'fihanllx', 'bfianllyt', 'fianltvly', 'ficanlyy', 'fianljply', 'figgnlly', 'fianllsb', 'fxianzly', 'fijanllo', 'rfianllc', 'fiajnily', 'diadnlly', 'fianllxv', 'fiuanllgy', 'fiangldly', 'aianllby', 'fplanlly', 'fianrlyu', 'fiavllys', 'ljanlly', 'pnanlly', 'fianludy', 'wfiaklly', 'jfianldy', 'fianlio', 'fiandllyy', 'biainlly', 'fraally', 'fijnllq', 'fiaznley', 'fianlleny', 'ufianllp', 'fivanllpy', 'fiaynlmly', 'fvaylly', 'fianqllu', 'vufianlly', 'ufianmly', 'viagnlly', 'eianllpy', 'fhianlrly', 'fnianllsy', 'fidanklly', 'xianlldy', 'bfianlla', 'kihnlly', 'fvianllwy', 'franldly', 'fiahllyv', 'fiadnxlly', 'fitnlvy', 'fivnllyp', 'fianlbkly', 'yiabnlly', 'fiasblly', 'fiabntly', 'fianclld', 'faianllym', 'kieanlly', 'fyianlay', 'fikanylly', 'fridanlly', 'fiavllqy', 'zfianlhly', 'fcanlloy', 'fiabcnlly', 'fiacnllh', 'fmianljly', 'fioaully', 'vrianlly', 'fyanlmly', 'tijanlly', 'fiaojlly', 'yfianllny', 'fvanwly', 'fiaqnllyf', 'fianllcmy', 'fwanllg', 'fxanllzy', 'fianlfz', 'fdqianlly', 'friahnlly', 'fkianllyh', 'fianllwiy', 'cfianllxy', 'fianollv', 'tfianllz', 'fiangey', 'fiqanllye', 'fianllzfy', 'fcifnlly', 'filzlly', 'mianlhy', 'xiqanlly', 'dianlls', 'arianlly', 'fgiacnlly', 'fialvlly', 'wianlcy', 'fianqllm', 'fiannnlly', 'finaxly', 'fianlliz', 'fijnllyo', 'fidanllvy', 'fianllyxs', 'fimnlly', 'bfianlrly', 'fianblljy', 'sfianllyo', 'nfianlsly', 'fiaolaly', 'fgawlly', 'fdaxlly', 'veianlly', 'fialllyl', 'fianlbuy', 'fbanlyly', 'faxlly', 'fianpldy', 'fianlfyq', 'fiayvly', 'pianllw', 'tfianllyg', 'jfxianlly', 'bfianvly', 'fiantfy', 'fvalnly', 'fiaznllm', 'fuiwanlly', 'fianllwye', 'fpenlly', 'fibnllf', 'yfianlvly', 'fianllgz', 'faianlljy', 'dfiaelly', 'fianyllye', 'faanllvy', 'yfianllc', 'fiynlkly', 'llanlly', 'fiangls', 'siannly', 'fyanllyp', 'fipnllby', 'fikanlily', 'fiayqnlly', 'fiavntlly', 'fihnllty', 'viazlly', 'fyianlmly', 'wfianllr', 'fiamllye', 'fianllady', 'fgianlpy', 'ufianally', 'fianlilky', 'kfialnlly', 'fianvlym', 'tfianley', 'kianliy', 'qianlyy', 'fianlpjy', 'bimnlly', 'fpaanlly', 'efianly', 'ianyly', 'fralnlly', 'fiyanplly', 'fianlwhly', 'gbanlly', 'jfaanlly', 'iiadnlly', 'afkanlly', 'finanllyy', 'fiynllx', 'fianlhpy', 'fianlcy', 'tiaflly', 'fianqlk', 'fiagnlle', 'fianllyhj', 'ficanlky', 'fiaknolly', 'hfianply', 'fiatmnlly', 'fracnlly', 'fiayylly', 'fibvnlly', 'tianuly', 'fiuasnlly', 'fipenlly', 'fiabnslly', 'fiaznlyy', 'franllu', 'fianblyv', 'sfiasnlly', 'faianlrly', 'fiayllg', 'fianljry', 'nfitnlly', 'figarnlly', 'fiaxnllyx', 'finanlll', 'fianolll', 'jiatnlly', 'faianllv', 'fiaoqly', 'fuiinlly', 'fiaynlky', 'fianlplby', 'fiaxnllqy', 'sfianoly', 'fvianllj', 'ifkianlly', 'fiadnllry', 'qfianylly', 'fiuanlaly', 'fiawlply', 'fgianrly', 'ciacnlly', 'firblly', 'fianlpcly', 'fianinly', 'fciunlly', 'finaknlly', 'filanlaly', 'wfianllm', 'fianwyl', 'afianlkly', 'gianllys', 'fianellyv', 'qianllv', 'fisanllyi', 'fhangly', 'ifihnlly', 'fisnllsy', 'fmonlly', 'sianllry', 'fiagxly', 'fiwanley', 'iiaclly', 'ciaznlly', 'fzianlhy', 'fianlrlry', 'fiadxlly', 'fianljlby', 'fiaknlliy', 'fianlplky', 'fianllfyg', 'fixlly', 'fifancly', 'fiazflly', 'fcnianlly', 'fiaonvlly', 'fieanwly', 'fiadnlrly', 'fuanlwy', 'fiazlny', 'finnlli', 'fianqlyh', 'filnllyx', 'ftiahlly', 'fianlylj', 'bfiranlly', 'fiaeelly', 'tiunlly', 'fianleyb', 'fiabllyr', 'fianllli', 'fianglley', 'mfianllx', 'fpaknlly', 'fianllybw', 'fianvmlly', 'fibanlzly', 'fiagnully', 'fianllafy', 'fziainlly', 'fieclly', 'fianlyh', 'fjianllyi', 'fdaslly', 'gfiamlly', 'fihglly', 'cfiannly', 'fixnlgy', 'fiavolly', 'fiaynllyv', 'yianllf', 'efiaznlly', 'fianslz', 'fianlwky', 'vfianplly', 'fiinlaly', 'rfainlly', 'fgianllt', 'cianlljy', 'jianloly', 'fiemnlly', 'ifianwlly', 'mianllo', 'fiatgly', 'ifianllry', 'vfianlcy', 'cfignlly', 'fmianjly', 'ifanhlly', 'fianqtly', 'fidanxlly', 'rfianlfy', 'fiayglly', 'mflanlly', 'fiwaally', 'fiangllyd', 'flianmly', 'mfianlli', 'fieanllyt', 'feanlly', 'bianlny', 'fiarlcy', 'hfvanlly', 'filanwly', 'fvianlcy', 'fiapnlyy', 'hfianlqy', 'fisanllyo', 'fidnlloy', 'fpianlvy', 'fiaenply', 'pfianlmly', 'faanwlly', 'fianblv', 'fiannyy', 'kiianlly', 'fianmllky', 'finnllp', 'friianlly', 'rdianlly', 'fianclrly', 'djanlly', 'nfxianlly', 'fiaonluly', 'fsianllym', 'ftiandly', 'fiaunluy', 'lfiahnlly', 'mfianhly', 'lfianllhy', 'ffianply', 'fianmluly', 'fimvnlly', 'fianlylny', 'qpanlly', 'dianlyly', 'fianllzyw', 'fianlklyf', 'fiabnllye', 'fiaqnllp', 'cmanlly', 'fianllxxy', 'pianlmly', 'fiandllay', 'feianllyq', 'dianaly', 'fisnjly', 'fiyknlly', 'ofiallly', 'wfaianlly', 'ebanlly', 'fiunllyd', 'qfitanlly', 'miatlly', 'fienlqy', 'fiwanmlly', 'jfiaynlly', 'fiasnply', 'foaianlly', 'fkianuly', 'fianhllv', 'fiaqnlljy', 'ofianlkly', 'fiabllyv', 'vfiaznlly', 'fianliby', 'sfianxlly', 'fiufnlly', 'fihhlly', 'fianylliy', 'yfiaclly', 'sfianslly', 'fiacnlwy', 'fwianllcy', 'figanlxly', 'fianollgy', 'fianwlxy', 'diranlly', 'wfiantly', 'fiapxly', 'hfivanlly', 'fianbllyy', 'uiapnlly', 'zfiagnlly', 'fidanllxy', 'fianlqlyz', 'fipanlyly', 'fmanlxly', 'fkiaflly', 'cfqianlly', 'fianlrmy', 'fciahnlly', 'firanzlly', 'blianlly', 'fsaanlly', 'fiejanlly', 'fiaelhly', 'fianlvley', 'fiandfy', 'fianzllb', 'fiaioly', 'fianlefy', 'fiaonclly', 'fikanllyd', 'fiandllye', 'fianlldyg', 'flanllyt', 'fiuznlly', 'fiinqly', 'jfianlmy', 'fiuaplly', 'fianlofy', 'fiabllry', 'fixnuly', 'ffianllyq', 'fiaynplly', 'ofianlfy', 'lfiawnlly', 'fianrlye', 'fianlgle', 'fianlulsy', 'fiarllx', 'fianklely', 'fiaelgy', 'xianlty', 'fnanlll', 'fzianllzy', 'fianvxy', 'fiandlfy', 'feianllg', 'fshnlly', 'wfianqlly', 'fianlquly', 'fiaxbnlly', 'fianlclyr', 'oiaelly', 'frianyly', 'fiynlxly', 'fianfllyj', 'fiknlla', 'fiarnllay', 'hianplly', 'fianlvmly', 'fianllqyo', 'wfianluly', 'oianltly', 'jilanlly', 'fiallley', 'fiatlhly', 'fianvqy', 'fiancdy', 'fiatnloly', 'ifanwly', 'vianllyq', 'fivaonlly', 'fianlyo', 'fianplliy', 'qbanlly', 'finagnlly', 'fiarllc', 'wianoly', 'ianolly', 'vfianlily', 'fianxqy', 'figanlny', 'pianlvy', 'fianwnly', 'fianqla', 'fpanfly', 'oinlly', 'fianlolty', 'fyanllf', 'fisnlqy', 'fianvlyo', 'fwianllyz', 'fianllbm', 'fianelyp', 'fwanlaly', 'fifanlxly', 'dfianlyl', 'feanloy', 'tianely', 'fianlklyg', 'fsianlld', 'finanllty', 'fiangldy', 'fianlilf', 'firnllpy', 'fiknqlly', 'fzianlky', 'efianllpy', 'fiktlly', 'fijnlely', 'ffianwlly', 'fianblp', 'fodnlly', 'finaljy', 'faianlky', 'fimandlly', 'sianlsy', 'aiaqnlly', 'fifndly', 'fvanlyly', 'pfianlky', 'nsianlly', 'xiaelly', 'wficnlly', 'fxianlle', 'fqanllz', 'fsanclly', 'fieanllyr', 'yfianqly', 'fiancvy', 'fianlolyr', 'feaally', 'fiahlqy', 'kfiknlly', 'rfianllyd', 'fmiacnlly', 'fialnlby', 'fqhianlly', 'fczanlly', 'fiaalyly', 'pfianlily', 'fiaxhlly', 'fiuaklly', 'fiannllyt', 'ofianllqy', 'fimngly', 'fifanjlly', 'fianlltf', 'fiagnllys', 'fianslyk', 'fiknllj', 'fiandlle', 'fianlliya', 'qianlldy', 'ofianlluy', 'fianlhym', 'kfbanlly', 'fifnlwly', 'zianllty', 'fkipnlly', 'fiatnlwly', 'fianzply', 'jfranlly', 'fgiangly', 'zfoanlly', 'fniajlly', 'xnanlly', 'fvanllj', 'nfianllyd', 'rfiaynlly', 'faianllyt', 'gfiancly', 'fianlklny', 'fsanmlly', 'fiaenlyl', 'fianllcyu', 'nfianllb', 'sfiaznlly', 'afianlly', 'xibnlly', 'zianlpy', 'xfianllb', 'lfiamlly', 'wianlln', 'fwanllf', 'fiaonlmly', 'fiajnllxy', 'fioaqlly', 'bianllyp', 'yiawnlly', 'fiazndlly', 'feaully', 'finllyq', 'liangly', 'vgfianlly', 'fianlelp', 'bianlwly', 'fyanllyb', 'fialnllyk', 'oianfly', 'fioanflly', 'fianqoly', 'fiabltly', 'ftanllsy', 'fianllhhy', 'fnanwly', 'fiaanlmly', 'tfianllye', 'fimaunlly', 'fianjilly', 'sfianlvy', 'jfialnlly', 'fianxmy', 'fiiclly', 'fifanlli', 'fiqaelly', 'kfiaenlly', 'miawlly', 'fiaslxly', 'fpianlqly', 'fsibnlly', 'fignlrly', 'fuannly', 'fianlrye', 'fiaqnally', 'fiamllyp', 'fiaplli', 'fihagnlly', 'fsianlaly', 'ffitnlly', 'ffanllwy', 'fianlfyn', 'faianluly', 'fiqagnlly', 'fipanllv', 'ficanlla', 'feipanlly', 'fcixanlly', 'fgianlry', 'fianlplys', 'fmaqnlly', 'ifanlny', 'vianlluy', 'fibanjly', 'fianxily', 'fianlklyp', 'fionlpy', 'rfbanlly', 'fiianqlly', 'fianwllyf', 'fgnanlly', 'fianllmdy', 'dfjianlly', 'fiantlw', 'fiwlnlly', 'fianhllyr', 'pianqlly', 'fianlxlyd', 'tjfianlly', 'ofqianlly', 'vilanlly', 'lofianlly', 'fianzlkly', 'fmiancly', 'fhanylly', 'fqiallly', 'fianallyb', 'fakanlly', 'fyanyly', 'fikylly', 'fianzlj', 'yfranlly', 'fianlldby', 'flianhlly', 'filnllz', 'fifnlld', 'mfiazlly', 'fwianluy', 'fianolyy', 'yfimanlly', 'fizaylly', 'fiaclhy', 'fwjanlly', 'fiagllgy', 'fisaxlly', 'fiadnlxly', 'fgyianlly', 'tianllyk', 'jcianlly', 'ifanluly', 'finallyk', 'fianpll', 'ffiaonlly', 'fognlly', 'fiaulgy', 'efianlzly', 'fiaynlyly', 'oianply', 'fiapnlry', 'fiantllyw', 'fwasnlly', 'fkianlwy', 'eianllyo', 'feiankly', 'fiwnllj', 'fiawllvy', 'jianllyu', 'fiandllx', 'ciamnlly', 'fiangpy', 'ufianllv', 'fkanhly', 'fignllr', 'zianllyh', 'ftiznlly', 'fvianllya', 'fiunllm', 'fianllyyn', 'fanllxy', 'jianllyc', 'wfianllyy', 'fianfllty', 'uqianlly', 'uflianlly', 'fianllywt', 'fiqnoly', 'fianllyzg', 'fixanlyy', 'fianplyz', 'fipnplly', 'fuionlly', 'ziarlly', 'frianllg', 'flanlnly', 'fainllyc', 'fiaqaly', 'fifanlzy', 'fiahlmy', 'filanply', 'ficavlly', 'fijfnlly', 'dfianlfy', 'fjanljly', 'hfianllfy', 'fianlglym', 'uianllly', 'fciallly', 'fdaonlly', 'fianhlj', 'fqanlly', 'giianlly', 'fipnllp', 'yfianllp', 'fiadnllzy', 'fiinllyk', 'fqanlsy', 'fiagnwly', 'fgiaolly', 'lfiwnlly', 'franqly', 'fiunlhly', 'ofaianlly', 'fliazlly', 'fianlsg', 'exanlly', 'fdyanlly', 'ffsanlly', 'fhiavnlly', 'pfianllyl', 'fbianlgly', 'fiailqy', 'fhiadnlly', 'fianfylly', 'fiaunllt', 'fyiafnlly', 'faianlley', 'fiandl', 'fhiknlly', 'fiadllyo', 'fiaonllhy', 'cbfianlly', 'fianzwy', 'yianlliy', 'wiaylly', 'friranlly', 'fianidy', 'fihanllxy', 'fianfyl', 'fianyls', 'finnllu', 'viantlly', 'fifnlay', 'fiyclly', 'niaglly', 'fkianllzy', 'yfiwnlly', 'oianllyw', 'fhaplly', 'fbranlly', 'fifbanlly', 'ftianllcy', 'fianllnf', 'fianlolly', 'fyanllyw', 'fianollfy', 'fiknsly', 'fianlglny', 'fjianllyt', 'faanllyw', 'rianlgly', 'bpianlly', 'fifqlly', 'fioanclly', 'yianley', 'sianlqly', 'nfiafnlly', 'iipnlly', 'fiyanlgy', 'fienply', 'fuanllj', 'fizanljy', 'xfixnlly', 'fiahndly', 'fimnll', 'hiranlly', 'fjagnlly', 'fijkanlly', 'fibanklly', 'fianlxr', 'bfijnlly', 'frandly', 'firngly', 'fibanlply', 'fdianzlly', 'wfqanlly', 'cfianlbly', 'fianlzxy', 'lfidnlly', 'fianlcb', 'fiatnsly', 'lfijnlly', 'fitnkly', 'fiagnllye', 'aianlty', 'fidanllf', 'fciganlly', 'vfianllyx', 'tfianelly', 'faianllk', 'ciaplly', 'fiywnlly', 'fifanlyl', 'xianllyi', 'fizanlwy', 'hfiaonlly', 'fibatlly', 'jiaenlly', 'kiknlly', 'sfhianlly', 'fianmlr', 'fcianally', 'fianullr', 'fiarnllr', 'fianlblyg', 'fianjlky', 'fiaqldly', 'fidjnlly', 'fiknljy', 'ftanwlly', 'bfiavnlly', 'fiatnllxy', 'fihaqnlly', 'fianlwpy', 'fiadllyj', 'ficanlzy', 'ooanlly', 'fionllg', 'qfianlkly', 'fiafllyr', 'fyianlley', 'fiapnllk', 'fiqnlli', 'fjeianlly', 'fmignlly', 'fibaclly', 'zfianllvy', 'fianazy', 'fiznlbly', 'fxianllyl', 'fiaenylly', 'zfiajlly', 'fianlvlk', 'fbianllhy', 'fganly', 'sfiatnlly', 'fianlcty', 'fianlkg', 'yianlnly', 'fiarnqlly', 'fridnlly', 'fiamnlhy', 'kianllk', 'qfianply', 'fuianlply', 'affianlly', 'fiadnlld', 'fwanluy', 'fikanlaly', 'fiakllx', 'fianllyzt', 'fiaellyv', 'cfiwanlly', 'fianlilg', 'zfiaglly', 'fviqanlly', 'ciuanlly', 'fiunllyz', 'jianllyp', 'fnanllqy', 'fianfllw', 'figahlly', 'afianlty', 'fiatnllw', 'fimallly', 'wfianllzy', 'fianqlc', 'finallmy', 'fianqllyn', 'fiaqnwlly', 'iffnlly', 'fibanlfy', 'jianllpy', 'niatnlly', 'gianlll', 'fhallly', 'fiqanyly', 'fianlkyj', 'fiawnllb', 'fiapqlly', 'fsianslly', 'fiadllyv', 'fianwlmly', 'fianlxlyj', 'fiaenljly', 'fiqqlly', 'fialnluly', 'afijanlly', 'fianllvyf', 'fiabyly', 'fieanllyp', 'fiantllxy', 'fvianllyv', 'fiwanrlly', 'fialnlwly', 'fihangly', 'gfeianlly', 'fianlile', 'fiasnwly', 'fjianllj', 'fiwanlfy', 'fjanllyc', 'fianlxlqy', 'fiafllxy', 'kfianllx', 'fisanlwy', 'fiaunllyh', 'qfiainlly', 'fjanlty', 'fidlnly', 'kianbly', 'fxcianlly', 'fiaallyz', 'flnianlly', 'fianyjlly', 'fsianlty', 'fianqlfy', 'fianlmqly', 'fidnvlly', 'fianltlyn', 'ifilnlly', 'fiwaplly', 'fiaonfly', 'fianekly', 'fianplaly', 'fiantlkly', 'fiaillyv', 'sfiangly', 'fihanllyy', 'fiacnlluy', 'pfianelly', 'fianullky', 'fianzlzly', 'hfianlcy', 'vxianlly', 'fianillye', 'fisanlhly', 'fikanllxy', 'fianfluy', 'fianllytw', 'fiaillry', 'frmianlly', 'fzkianlly', 'fianlbyb', 'fxiaplly', 'fianlzyd', 'flanmly', 'fianllsoy', 'yfianllyx', 'fibansly', 'cianlgy', 'fwanlzy', 'fiamllq', 'fianejly', 'vfwanlly', 'ftianllo', 'fianlkyy', 'fianklw', 'fianlblmy', 'fyiaznlly', 'fianlbdly', 'figenlly', 'fikanlby', 'firanllyh', 'bfianljy', 'wiaxlly', 'flianlfy', 'jially', 'finlhly', 'fiansully', 'fiexanlly', 'tfijanlly', 'foiaynlly', 'fiaqnllr', 'foianllfy', 'fiapnlkly', 'fiancty', 'mtianlly', 'pfianlvly', 'fioanllyk', 'filnllv', 'bianylly', 'ofianlwy', 'fiaplxy', 'fianhlely', 'fiqmlly', 'fiianwly', 'fidaclly', 'fiantoy', 'fiwalnly', 'lfuianlly', 'fvanllyk', 'nijanlly', 'fianlvlg', 'fpiaqlly', 'fianllpfy', 'fianvolly', 'divanlly', 'fibanyly', 'fiwwanlly', 'fiqnlry', 'fiantlv', 'lranlly', 'filbanlly', 'filglly', 'firpanlly', 'mfbianlly', 'fipanltly', 'fiarllyp', 'fiiajlly', 'fianhliy', 'fiaallhy', 'faaklly', 'wianfly', 'cfihanlly', 'fianlugy', 'focanlly', 'biawlly', 'fianllfd', 'fivanllp', 'fjignlly', 'fiwnqly', 'fianllkyu', 'ftianllyz', 'okfianlly', 'fjifnlly', 'fianvey', 'fsimnlly', 'ftanlvly', 'firanllny', 'fialnlyy', 'hianply', 'rfianloy', 'fignlloy', 'fsianllny', 'fiaelluy', 'jfialnly', 'fhanllyw', 'fitnlxy', 'fiqnllp', 'yianllr', 'filanllyp', 'ofvanlly', 'fvianlmly', 'fzanllg', 'fianhllq', 'fianmllg', 'fiaznslly', 'hfiandlly', 'lfianelly', 'feiapnlly', 'fioanll', 'fjiinlly', 'yfianaly', 'friavnlly', 'fifnlll', 'fizanllyg', 'fiaunlyly', 'ofianuly', 'franjly', 'fijnllh', 'fiainjly', 'fmianlsly', 'fixnllys', 'ficnlly', 'fiarxly', 'fianllayt', 'fiznjly', 'fianklyc', 'fitanlwy', 'fiaihlly', 'ftianxly', 'fianlwll', 'pfiaqlly', 'fianlhqly', 'fiamllyv', 'firzlly', 'fiaynllty', 'eianfly', 'fjianxlly', 'fianlnly', 'zhianlly', 'dianllw', 'feanllyy', 'fimnllqy', 'fxanllyg', 'firranlly', 'fmanclly', 'fiawvlly', 'fiaillyw', 'fiaynslly', 'fiaklny', 'bfianllyq', 'fcainlly', 'foiwanlly', 'fianyl', 'afianltly', 'fiadnllcy', 'finalxy', 'sidanlly', 'fniknlly', 'fianlayx', 'fianocy', 'fianllpyq', 'ianllpy', 'fianytlly', 'fiajklly', 'fianrllyr', 'fianljlq', 'ftahlly', 'fidnyly', 'efianllyt', 'faianclly', 'fianljlmy', 'fiiajnlly', 'filylly', 'fianlqyu', 'fiaklgy', 'sbianlly', 'fwamlly', 'fioanllny', 'fkanlyly', 'feianlloy', 'mianlily', 'fiiatlly', 'fifanldy', 'fibnllgy', 'figllly', 'kianlpy', 'qianglly', 'fixantlly', 'fally', 'fkanllzy', 'fianlisly', 'tianlky', 'fiaflmy', 'fianlmjy', 'mfijanlly', 'jfianlry', 'yialnly', 'fieanllk', 'fkanlhly', 'fiwolly', 'lfianllm', 'fiarnlfy', 'fiunply', 'fiawnllye', 'fidnlzly', 'uiknlly', 'fiaalmy', 'fiapllf', 'zfianlll', 'fiandrly', 'ifanlluy', 'fianlwry', 'nianllq', 'fibanlbly', 'fianlljw', 'ciaonlly', 'fiaunwlly', 'ufianylly', 'fiahliy', 'filully', 'fianilhly', 'fianllyui', 'fianclk', 'ftipnlly', 'fyiancly', 'biacnlly', 'zfianldy', 'fianllyov', 'fiaqnnly', 'ufiansly', 'fianlrdy', 'fihanlloy', 'ftiaynlly', 'fhjianlly', 'fmanlily', 'fixanlfly', 'lnianlly', 'fsiangly', 'fianlqt', 'fwianlny', 'fiounlly', 'finallya', 'fixanllyy', 'fdanllky', 'flianluy', 'fizanlcly', 'fqiaanlly', 'fiayqlly', 'fiamlky', 'mianlny', 'fianlfqy', 'fyianlyl', 'fiaenllyn', 'fimnllc', 'fiawnylly', 'flianllpy', 'fiaxnllry', 'fiinllf', 'fipnlxly', 'fbianllyc', 'fianllhyd', 'fqianlgy', 'fiawllwy', 'ficfnlly', 'ffzanlly', 'xianlgy', 'fianjlyt', 'zfiaznlly', 'fiaqllyp', 'fiaxnllzy', 'hianily', 'fionllk', 'fiacnklly', 'afianlluy', 'fiangllby', 'fianllcyb', 'fiaglqy', 'kianhlly', 'bfrianlly', 'finanllyh', 'fitailly', 'fianllbe', 'fiuanlgy', 'fianvllh', 'fiasnally', 'fpianllgy', 'iialnly', 'fianldyz', 'fiasllty', 'fiensly', 'fianldlmy', 'fianmfy', 'fiacylly', 'nfially', 'fianlljys', 'dfiannly', 'feiavnlly', 'fiahllyq', 'pfiaynlly', 'fviabnlly', 'fiaxsnlly', 'fianlvky', 'fiaznvly', 'nionlly', 'flianqlly', 'uianlcy', 'piaelly', 'fbianllty', 'fianejlly', 'fliacnlly', 'fiaavnlly', 'fjanllty', 'fiaeqlly', 'fianxllry', 'fviwanlly', 'fyianllz', 'fianlill', 'finanlvly', 'foiaully', 'jianlqly', 'fiqnllby', 'fnianllyp', 'yfiaynlly', 'fianlryd', 'fixmnlly', 'fianedly', 'fiuqnlly', 'fuanxly', 'fiaonlxy', 'fianbliy', 'fianllau', 'fwaklly', 'fidanully', 'fifnldly', 'fiwnoly', 'fianslljy', 'fianllgm', 'ficnclly', 'bianlqy', 'vqfianlly', 'fihanljy', 'xfiamnlly', 'fimnslly', 'fiacnully', 'fqapnlly', 'dfinanlly', 'fianluz', 'rfixanlly', 'fixnolly', 'fianlypl', 'ftsanlly', 'fiunllyl', 'fianklcly', 'ifanllqy', 'fianalyly', 'fikanlli', 'pfianloy', 'fbianxlly', 'fvanlfly', 'fianlwlyh', 'fianllcv', 'fianelfly', 'fianils', 'fiawllny', 'efiaflly', 'fiapnlmly', 'fiinlwly', 'fianlzi', 'fiynllw', 'fvianlla', 'wfianllj', 'fiaylcy', 'nfranlly', 'xfianluy', 'flianllyr', 'fianqlley', 'foanbly', 'fivnloy', 'ftianllyi', 'ianplly', 'fhiaylly', 'flanllq', 'fiedlly', 'fianlalh', 'fianlled', 'ficnlmy', 'fpiamnlly', 'ffianilly', 'pfianluy', 'fqqnlly', 'fianlgvy', 'rianuly', 'ftianlby', 'fidnrlly', 'fignllhy', 'fianfky', 'fianclls', 'fyiaxnlly', 'fianlwyf', 'ifiaclly', 'xfianllo', 'fiaynll', 'vfianbly', 'fiqanzly', 'fiainlaly', 'fienjlly', 'fianlayj', 'finkanlly', 'fbiaflly', 'fhiwnlly', 'fiabllyw', 'fiaqlly', 'fianlleoy', 'miankly', 'fianllpyr', 'gianlsly', 'fiwnlgy', 'fisaklly', 'fpcnlly', 'figxnlly', 'faianloly', 'aianlle', 'jianllya', 'fianillpy', 'aiarnlly', 'jfiantlly', 'fiawncly', 'fianllylx', 'fiarnsly', 'fikslly', 'fuaenlly', 'fiapnglly', 'fiangllty', 'fiaulwly', 'ifvanlly', 'fianlzc', 'kianlhly', 'fiaolli', 'oianljy', 'zfiabnlly', 'fiyanllcy', 'fiznplly', 'fiaclvly', 'fpanllye', 'fianplyi', 'sfianely', 'fcianlky', 'fiuanllmy', 'fianrely', 'fmannly', 'flianljly', 'fkianlmly', 'fiaatly', 'fioanllq', 'fiaqloy', 'fianzlley', 'fidaenlly', 'fiaynlay', 'mfwianlly', 'ufianllsy', 'ziandly', 'yianllyu', 'fwianllyl', 'filrnlly', 'fianlglry', 'fgdanlly', 'pnfianlly', 'fijnllyf', 'fyanllye', 'fiknxlly', 'aianllyq', 'tianllyc', 'fvianllzy', 'fidanllry', 'fyiaully', 'ofkanlly', 'fianlalu', 'fizanldly', 'fiannlyd', 'fianjli', 'fjanuly', 'faianmly', 'fiafnoly', 'fivanllyn', 'fdanllyl', 'fiahnlwy', 'fianlnvly', 'fioanlby', 'fzanllcy', 'fianllhd', 'fiaznllyv', 'fifanllpy', 'yfiailly', 'kfianxly', 'fignlyy', 'fianillyp', 'fidanlmly', 'rfiafnlly', 'fihoanlly', 'fianolyz', 'ftanxlly', 'fianwully', 'fivanlty', 'fzaianlly', 'fiawllyw', 'fiaflwy', 'fiawnrly', 'goanlly', 'ifasnlly', 'fiawzly', 'fianllpdy', 'fkanllyg', 'faianllyl', 'fianlxwly', 'efmanlly', 'fyanllyx', 'flitanlly', 'fiamnmly', 'fianvllr', 'fuxianlly', 'fianlrlyg', 'fianonly', 'friaally', 'sfiandlly', 'wfiaylly', 'fiouanlly', 'fiaynlgy', 'efwanlly', 'fianollvy', 'xianllya', 'fianquly', 'lfianlley', 'fiianljy', 'fianllwuy', 'rianlli', 'fibznlly', 'fiatnlpy', 'figylly', 'fainlyy', 'fnianllt', 'faanllcy', 'gianlyly', 'fianzlljy', 'ffianllj', 'efianllyd', 'fiaxnloy', 'fyanllgy', 'fbaolly', 'fbanlfy', 'fihanmlly', 'ficnwly', 'franlvy', 'fikanlll', 'tfianlmly', 'fianlilb', 'xxanlly', 'fianplyv', 'finallx', 'fianlolby', 'pfianllr', 'fianqllyu', 'wianllys', 'filanllyi', 'fianlmlcy', 'fyiunlly', 'fvanlky', 'bfianllxy', 'fiafllyz', 'fsqnlly', 'qienlly', 'qianqly', 'iaplly', 'fitanllu', 'fyaclly', 'fianllcyn', 'fianqmlly', 'hianlty', 'qfiacnlly', 'fqianzlly', 'fiabhly', 'fianlholy', 'lkianlly', 'fmtanlly'}\n",
      "{'frankly', 'finally', 'finely', 'faintly'}\n",
      "{'finally'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def tokens(text): \n",
    "    \"\"\"\n",
    "    Get all words from the corpus\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "WORDS = tokens(open('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "# top 10 words in corpus\n",
    "print('Top 10 words in corpus:')\n",
    "print(WORD_COUNTS.most_common(10), '\\n')\n",
    "\n",
    "def edits0(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away \n",
    "    from the input word (i.e., the word itself).\n",
    "    \"\"\"\n",
    "    return {word}\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edit away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        Return a list of all possible (first, rest) pairs \n",
    "        that the input word is made of.\n",
    "        \"\"\"\n",
    "        return [(word[:i], word[i:]) \n",
    "                for i in range(len(word)+1)]\n",
    "                \n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"Return all strings that are two edits away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "\n",
    "def known(words):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are actually\n",
    "    in our WORD_COUNTS dictionary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in WORD_COUNTS}\n",
    "\n",
    "print('Input words:')\n",
    "# input word\n",
    "word = 'fianlly'\n",
    "\n",
    "# zero edit distance from input word\n",
    "print(edits0(word))\n",
    "# returns null set since it is not a valid word\n",
    "print(known(edits0(word)))\n",
    "# one edit distance from input word\n",
    "print(edits1(word))\n",
    "# get correct words from above set\n",
    "print(known(edits1(word)))\n",
    "# two edit distances from input word\n",
    "print(edits2(word))\n",
    "# get correc twords from above set\n",
    "print(known(edits2(word)))\n",
    "candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "print(candidates, '\\n')"
   ]
  },
  {
   "source": [
    "## Spelling Correction Part 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finally\n",
      "FIANLLY \n",
      "\n",
      "finally\n",
      "FINALLY \n",
      "\n",
      "TextBlob way (you may need to use pip to install textblob):\n",
      "finally\n",
      "[('finally', 1.0)]\n",
      "[('flat', 0.85), ('float', 0.15)]\n"
     ]
    }
   ],
   "source": [
    "def correct(word):\n",
    "    '''\n",
    "    Get the best correct spelling for the input word\n",
    "    :param word: the input word\n",
    "    :return: best correct spelling\n",
    "    '''\n",
    "    # priority is for edit distance 0, then 1, then 2\n",
    "    # else defaults to the input word iteself.\n",
    "    candidates = (known(edits0(word)) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)\n",
    "\n",
    "print(correct('fianlly'))\n",
    "print(correct('FIANLLY'), '\\n')\n",
    "\n",
    "def correct_match(match):\n",
    "    '''\n",
    "    Spell-correct word in match, and preserve proper upper/lower/title case.\n",
    "    :param match: word to be corrected\n",
    "    :return: corrected word\n",
    "    '''\n",
    "    word = match.group()\n",
    "    def case_of(text):\n",
    "        '''\n",
    "        Return the case-function appropriate for text: upper/lower/title/as-is\n",
    "        :param text: The text to be acted on\n",
    "        :return: Correct text\n",
    "        '''\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def correct_text_generic(text):\n",
    "    '''\n",
    "    Correct all the words within a text, returning the corrected text\n",
    "    :param text: Text to be corrected\n",
    "    :return: Corrected text\n",
    "    '''\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "print(correct_text_generic('fianlly'))\n",
    "print(correct_text_generic('FIANLLY'), '\\n')\n",
    "\n",
    "print('TextBlob way (you may need to use pip to install textblob):')\n",
    "from textblob import Word\n",
    "w = Word('fianlly')\n",
    "print(w.correct())\n",
    "# check suggestions\n",
    "print(w.spellcheck())\n",
    "# another example\n",
    "w = Word('flaot')\n",
    "print(w.spellcheck())"
   ]
  },
  {
   "source": [
    "## Stem and Lem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Porter stemmer:\n",
      "jump jump jump\n",
      "lie\n",
      "strang \n",
      "\n",
      "Lancaster stemmer:\n",
      "jump jump jump\n",
      "lying\n",
      "strange \n",
      "\n",
      "Regex stemmer:\n",
      "jump jump jump\n",
      "ly\n",
      "strange \n",
      "\n",
      "Snowball stemmer:\n",
      "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "autobahn\n",
      "spring \n",
      "\n",
      "WordNet lemmatization:\n",
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n",
      "ate\n",
      "fancier \n",
      "\n",
      "spaCy:\n",
      "My system keep crash ! his crash yesterday , ours crash daily\n"
     ]
    }
   ],
   "source": [
    "# porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print('Porter stemmer:')\n",
    "print(ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))\n",
    "print(ps.stem('lying'))\n",
    "print(ps.stem('strange'), '\\n')\n",
    "\n",
    "# lancaster stemmer\n",
    "print('Lancaster stemmer:')\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "print(ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
    "print(ls.stem('lying'))\n",
    "print(ls.stem('strange'), '\\n')\n",
    "\n",
    "# regex stemmer\n",
    "print('Regex stemmer:')\n",
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "print(rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))\n",
    "print(rs.stem('lying'))\n",
    "print(rs.stem('strange'), '\\n')\n",
    "\n",
    "# snowball stemmer\n",
    "print('Snowball stemmer:')\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "print('Supported Languages:', SnowballStemmer.languages)\n",
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "print(ss.stem('autobahnen'))\n",
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "print(ss.stem('springen'), '\\n')\n",
    "\n",
    "# lemmatization\n",
    "print('WordNet lemmatization:')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))\n",
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'), '\\n')\n",
    "\n",
    "print('spaCy:')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'My system keeps crashing! his crashed yesterday, ours crashes daily'\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-'\n",
    "                     else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "print(lemmatize_text(text))"
   ]
  },
  {
   "source": [
    "## Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ", , stopwords , computer\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# removing stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "print(remove_stopwords('The, and, if are stopwords, computer is not'))"
   ]
  },
  {
   "source": [
    "## POS Tagging - Starting on Page 166"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spaCy:\n",
      "             Word POS tag Tag type\n",
      "0              US     NNP    PROPN\n",
      "1         unveils     VBZ     VERB\n",
      "2           world      NN     NOUN\n",
      "3              's     POS     PART\n",
      "4            most     RBS      ADV\n",
      "5        powerful      JJ      ADJ\n",
      "6   supercomputer      NN     NOUN\n",
      "7               ,       ,    PUNCT\n",
      "8           beats     VBZ     VERB\n",
      "9           China     NNP    PROPN\n",
      "10              .       .    PUNCT\n",
      "\n",
      " NLTK\n",
      "             Word POS tag\n",
      "0              US    NOUN\n",
      "1         unveils     ADJ\n",
      "2           world    NOUN\n",
      "3              's     PRT\n",
      "4            most     ADV\n",
      "5        powerful     ADJ\n",
      "6   supercomputer    NOUN\n",
      "7               ,       .\n",
      "8           beats    VERB\n",
      "9           China    NOUN\n",
      "10              .       .\n",
      "\n",
      " Treebank:\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      " Default tagger:\n",
      "0.1454158195372253\n",
      "[('US', 'NN'), ('unveils', 'NN'), ('world', 'NN'), (\"'s\", 'NN'), ('most', 'NN'), ('powerful', 'NN'), ('supercomputer', 'NN'), (',', 'NN'), ('beats', 'NN'), ('China', 'NN'), ('.', 'NN')]\n",
      "\n",
      " Regex tagger\n",
      "0.24039113176493368\n",
      "[('US', 'NN'), ('unveils', 'NNS'), ('world', 'NN'), (\"'s\", 'NN$'), ('most', 'NN'), ('powerful', 'NN'), ('supercomputer', 'NN'), (',', 'NN'), ('beats', 'NNS'), ('China', 'NN'), ('.', 'NN')]\n",
      "\n",
      " N Gram taggers\n",
      "0.8607803272340013\n",
      "[('US', 'NNP'), ('unveils', None), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'JJS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', None), ('China', 'NNP'), ('.', '.')]\n",
      "0.13466937748087907\n",
      "[('US', None), ('unveils', None), ('world', None), (\"'s\", None), ('most', None), ('powerful', None), ('supercomputer', None), (',', None), ('beats', None), ('China', None), ('.', None)]\n",
      "0.08064672281924679\n",
      "[('US', None), ('unveils', None), ('world', None), (\"'s\", None), ('most', None), ('powerful', None), ('supercomputer', None), (',', None), ('beats', None), ('China', None), ('.', None)]\n",
      "0.9094781682641108\n",
      "[('US', 'NNP'), ('unveils', 'NNS'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'NNS'), ('China', 'NNP'), ('.', '.')]\n",
      "\n",
      " Naive Bayes and Maxent\n",
      "0.9306806079969019\n",
      "[('US', 'PRP'), ('unveils', 'VBZ'), ('world', 'VBN'), (\"'s\", 'POS'), ('most', 'JJS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')] \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nmet = ClassifierBasedPOSTagger(train=train_data,\\n                               classifier_builder=MaxentClassifier.train)\\nprint(met.evaluate(test_data))\\nprint(met.tag(nltk.word_tokenize(sentence)))\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "sentence = 'US unveils world\\'s most powerful supercomputer, beats China.'\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "\n",
    "print('spaCy:')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence_nlp = nlp(sentence)\n",
    "# POS tagging with spaCy\n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "# the .T in the book transposes rows and columsn, but it's harder to read\n",
    "pprint(pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type']))\n",
    "\n",
    "# POS tagging with nltk\n",
    "print('\\n', 'NLTK')\n",
    "import nltk\n",
    "# only need the following two lines one time\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('universal_tagset')\n",
    "nltk_pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence), tagset='universal')\n",
    "pprint(pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag']))\n",
    "\n",
    "print('\\n', 'Treebank:')\n",
    "# you only need the next line once\n",
    "# nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])\n",
    "\n",
    "print('\\n', 'Default tagger:')\n",
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "# accuracy on test data\n",
    "print(dt.evaluate(test_data))\n",
    "# tagging our sample headline\n",
    "print(dt.tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "print('\\n', 'Regex tagger')\n",
    "# regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "# accuracy on test data\n",
    "print(rt.evaluate(test_data))\n",
    "# tagging our sample headline\n",
    "print(rt.tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "print('\\n', 'N Gram taggers')\n",
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "# testing performance on unigram tagger\n",
    "print(ut.evaluate(test_data))\n",
    "print(ut.tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "# testing performance of bigram tagger\n",
    "print(bt.evaluate(test_data))\n",
    "print(bt.tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "# testing performance of trigram tagger\n",
    "print(tt.evaluate(test_data))\n",
    "print(tt.tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data,\n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "print(ct.evaluate(test_data))\n",
    "print(ct.tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "print('\\n', 'Naive Bayes and Maxent')\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "print(nbt.evaluate(test_data))\n",
    "print(nbt.tag(nltk.word_tokenize(sentence)), '\\n')\n",
    "\n",
    "# the following takes a LONG time to run - run if you have time\n",
    "'''\n",
    "met = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=MaxentClassifier.train)\n",
    "print(met.evaluate(test_data))\n",
    "print(met.tag(nltk.word_tokenize(sentence)))\n",
    "'''"
   ]
  },
  {
   "source": [
    "## Shallow Parsing - Starting on Page 173\n",
    "\n",
    "This can take a bit of time to run at the end..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Treebank:\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.) \n",
      "\n",
      "Regext parser:\n",
      "POS Tags: [('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (NP China/NNP)\n",
      "  ./.) \n",
      "\n",
      "Chinking:\n",
      "(S\n",
      "  (NP\n",
      "    US/NNP\n",
      "    unveils/JJ\n",
      "    world/NN\n",
      "    's/POS\n",
      "    most/RBS\n",
      "    powerful/JJ\n",
      "    supercomputer/NN\n",
      "    ,/,\n",
      "    beats/VBZ\n",
      "    China/NNP\n",
      "    ./.)) \n",
      "\n",
      "More generic shallow parser:\n",
      "(S\n",
      "  (NP US/NNP)\n",
      "  (NP unveils/JJ world/NN)\n",
      "  's/POS\n",
      "  (ADVP most/RBS)\n",
      "  (NP powerful/JJ supercomputer/NN)\n",
      "  ,/,\n",
      "  (VP beats/VBZ)\n",
      "  (NP China/NNP)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%% \n",
      "\n",
      "Chunked and treebank:\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "[('A', 'DT', 'B-NP'), ('Lorillard', 'NNP', 'I-NP'), ('spokewoman', 'NN', 'I-NP'), ('said', 'VBD', 'O'), (',', ',', 'O'), ('``', '``', 'O'), ('This', 'DT', 'B-NP'), ('is', 'VBZ', 'O'), ('an', 'DT', 'B-NP'), ('old', 'JJ', 'I-NP'), ('story', 'NN', 'I-NP'), ('.', '.', 'O')]\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.) \n",
      "\n",
      "NGramTagChunker:\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.2%%\n",
      "    Precision:     91.4%%\n",
      "    Recall:        94.3%%\n",
      "    F-Measure:     92.8%%\n",
      "Parsing NTC...\n",
      "(S\n",
      "  (NP\n",
      "    US/NNP\n",
      "    unveils/JJ\n",
      "    world/NN\n",
      "    's/POS\n",
      "    most/RBS\n",
      "    powerful/JJ\n",
      "    supercomputer/NN)\n",
      "  ,/,\n",
      "  beats/VBZ\n",
      "  (NP China/NNP)\n",
      "  ./.)\n",
      "Wall Street Journal (cut to just 1000):\n",
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.3%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        86.2%%\n",
      "    F-Measure:     83.4%%\n"
     ]
    }
   ],
   "source": [
    "print('Treebank:')\n",
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[7], '\\n')\n",
    "\n",
    "print('Regext parser:')\n",
    "simple_sentence = 'US unveils world\\'s most powerful supercomputer, beats China.'\n",
    "from nltk.chunk import RegexpParser\n",
    "import nltk\n",
    "from pattern.en import tag\n",
    "# get POS tagged sentence\n",
    "tagged_simple_sent = nltk.pos_tag(nltk.word_tokenize(simple_sentence))\n",
    "print('POS Tags:', tagged_simple_sent)\n",
    "\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c, '\\n')\n",
    "\n",
    "print('Chinking:')\n",
    "chink_grammar = \"\"\"\n",
    "NP: {<.*>+} # chunk everything as NP\n",
    "}<VBD|IN>+{\n",
    "\"\"\"\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "# print and view chunked sentence using chinking\n",
    "print(c, '\\n')\n",
    "\n",
    "# create a more generic shallow parser\n",
    "print('More generic shallow parser:')\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "# print and view shallow parsed simple sentence\n",
    "print(c)\n",
    "# Evaluate parser performance on test data\n",
    "print(rc.evaluate(test_data), '\\n')\n",
    "\n",
    "print('Chunked and treebank:')\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "# get the (word, POS tag, Chung tag) triples for each token\n",
    "wtc = tree2conlltags(train_sent)\n",
    "print(wtc)\n",
    "# get shallow parsed tree back from the WTC trples\n",
    "tree = conlltags2tree(wtc)\n",
    "print(tree, '\\n')\n",
    "\n",
    "print('NGramTagChunker:')\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "  \n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n",
    "\n",
    "# train the shallow parser\n",
    "ntc = NGramTagChunker(train_data)\n",
    "# test parser performance on test data\n",
    "print(ntc.evaluate(test_data))\n",
    "\n",
    "# the next 2 lines don't belong and have been commented out\n",
    "# sentence_nlp = nlp(sentence)\n",
    "# tagged_sentence = [(word.text, word.tag_) for word in sentence_nlp]\n",
    "\n",
    "# parse our sample sentence\n",
    "print('Parsing NTC...')\n",
    "tree = ntc.parse(tagged_simple_sent)\n",
    "print(tree)\n",
    "tree.draw()\n",
    "\n",
    "print('Wall Street Journal (cut to just 1000):')\n",
    "# only need the next line once\n",
    "#nltk.download('conll2000')\n",
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:1000]\n",
    "test_wsj_data = wsj_data[1000:]\n",
    "print(train_wsj_data[10])\n",
    "\n",
    "# tran the shallow parser\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "# test performance on test data\n",
    "print(tc.evaluate(test_wsj_data))\n",
    "\n",
    "# there's code on the start of page 183 that's a repeat of the code on 181\n",
    "# I didn't even write it - no need"
   ]
  },
  {
   "source": [
    "## Dependency Parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]<---US[nsubj]--->[]\n",
      "--------\n",
      "['US']<---unveils[ROOT]--->['supercomputer', ',', 'beats', '.']\n",
      "--------\n",
      "[]<---world[poss]--->[\"'s\"]\n",
      "--------\n",
      "[]<---'s[case]--->[]\n",
      "--------\n",
      "[]<---most[advmod]--->[]\n",
      "--------\n",
      "['most']<---powerful[amod]--->[]\n",
      "--------\n",
      "['world', 'powerful']<---supercomputer[dobj]--->[]\n",
      "--------\n",
      "[]<---,[punct]--->[]\n",
      "--------\n",
      "[]<---beats[conj]--->['China']\n",
      "--------\n",
      "[]<---China[dobj]--->[]\n",
      "--------\n",
      "[]<---.[punct]--->[]\n",
      "--------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3486530ef1264cd485b8c58734c66a7f-0\" class=\"displacy\" width=\"950\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">US</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">unveils</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">world</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">'s</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">PART</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">most</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">ADV</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">powerful</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">supercomputer,</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">beats</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">China.</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PROPN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-0\" stroke-width=\"2px\" d=\"M70,202.0 C70,152.0 135.0,152.0 135.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,204.0 L64,194.0 76,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-1\" stroke-width=\"2px\" d=\"M270,202.0 C270,102.0 640.0,102.0 640.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M270,204.0 L264,194.0 276,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-2\" stroke-width=\"2px\" d=\"M270,202.0 C270,152.0 335.0,152.0 335.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M335.0,204.0 L341.0,194.0 329.0,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-3\" stroke-width=\"2px\" d=\"M470,202.0 C470,152.0 535.0,152.0 535.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M470,204.0 L464,194.0 476,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-4\" stroke-width=\"2px\" d=\"M570,202.0 C570,152.0 635.0,152.0 635.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M570,204.0 L564,194.0 576,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-5\" stroke-width=\"2px\" d=\"M170,202.0 C170,52.0 645.0,52.0 645.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M645.0,204.0 L651.0,194.0 639.0,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-6\" stroke-width=\"2px\" d=\"M170,202.0 C170,2.0 750.0,2.0 750.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M750.0,204.0 L756.0,194.0 744.0,194.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-3486530ef1264cd485b8c58734c66a7f-0-7\" stroke-width=\"2px\" d=\"M770,202.0 C770,152.0 835.0,152.0 835.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-3486530ef1264cd485b8c58734c66a7f-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M835.0,204.0 L841.0,194.0 829.0,194.0\" fill=\"currentColor\"/>\n</g>\n</svg>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sentence = 'US unveils world\\'s most powerful supercomputer, beats China.'\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence_nlp = nlp(sentence)\n",
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "for token in sentence_nlp:\n",
    "    print(dependency_pattern.format(word=token.orth_, w_type=token.dep_,\n",
    "                                    left=[t.orth_ for t in token.lefts],\n",
    "                                    right=[t.orth_ for t in token.rights]))\n",
    "                                             \n",
    "from spacy import displacy\n",
    "displacy.render(sentence_nlp, jupyter=True, style='dep',\n",
    "                options={'distance': 100,\n",
    "                        'arrow_stroke': 2,\n",
    "                        'arrow_width': 8})"
   ]
  },
  {
   "source": [
    "## NOTE\n",
    "The book goes into teh Stanford parser at the bottom of page 187. This Standford parser is depricated and requires a local server (too complicated for this). Therefore, I commented all the code out - it's just another parser and does the same thing as the rest of the code without the hassle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Constituency Parsing - Starting on Page 195"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .)) \n",
      "\n",
      "[NN -> 'traffic', NN -> 'creditworthiness', JJ -> 'other', VP -> VBD PP-MNR S, NN -> 'overtime', VBD -> 'mounted', VB -> 'suit', NP-SBJ -> WDT, NNS -> 'jumps', VB -> 'swap']\n",
      "[('US', 'NNP'), ('unveils', 'JJ'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), ('supercomputer', 'NN'), (',', ','), ('beats', 'VBZ'), ('China', 'NNP'), ('.', '.')] \n",
      "\n",
      "(S\n",
      "  (NP-SBJ-2\n",
      "    (NP (NNP US))\n",
      "    (NP\n",
      "      (NP (JJ unveils) (NN world) (POS 's))\n",
      "      (JJS most)\n",
      "      (JJ powerful)\n",
      "      (NN supercomputer)))\n",
      "  (, ,)\n",
      "  (VP (VBZ beats) (NP-TTL (NNP China)))\n",
      "  (. .)) (p=5.08954e-43)\n"
     ]
    }
   ],
   "source": [
    "entence = 'US unveils world\\'s most powerful supercomputer, beats China.'\n",
    "\n",
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "training_set = treebank.parsed_sents()\n",
    "print(training_set[1], '\\n')\n",
    "\n",
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "# view some production rules\n",
    "print(treebank_productions[0:10])\n",
    "  \n",
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "    t = nltk.Tree.fromstring( \"(\"+ tag + \" \" + word  + \")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n",
    "\n",
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "# get parse tree for sample sentence\n",
    "# this next lines throw and error (see the text on page 197)\n",
    "# result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "# get tokens and their POS tags and check it\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "print(tagged_sent, '\\n')\n",
    "\n",
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), treebank_productions)\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "#print parse tree\n",
    "print(result[0])\n",
    "# visualize parse tree\n",
    "result[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}