{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 7 - Summarizing Text\n",
    "## Keyphrase Extraction - Starting on Page 352\n",
    "This code just follows the text, with corrections from the book where necessary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting TextNormalizer\n",
      "Done strip\n",
      "Done lower\n",
      "Done stopword\n",
      "Done char remove\n",
      "Done contract exp\n",
      "Done spec char remove\n",
      "\n",
      "Alice - before and after\n",
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] \n",
      "  alice adventures wonderland lewis carroll  \n",
      "\n",
      "Bigrams:\n",
      " [('said alice', 123), ('mock turtle', 56), ('march hare', 31), ('said king', 29), ('thought alice', 26), ('white rabbit', 22), ('said hatter', 22), ('said mock', 20), ('said caterpillar', 18), ('said gryphon', 18)] \n",
      "\n",
      "Trigrams:\n",
      " [('said mock turtle', 20), ('said march hare', 10), ('poor little thing', 6), ('little golden key', 5), ('certainly said alice', 5), ('white kid gloves', 5), ('march hare said', 5), ('mock turtle said', 5), ('know said alice', 4), ('might well say', 4)]\n",
      "Collocation Finder:\n",
      "\n",
      "Bigram Association Measures:\n",
      "[('said', 'alice'), ('mock', 'turtle'), ('march', 'hare'), ('said', 'king'), ('thought', 'alice'), ('said', 'hatter'), ('white', 'rabbit'), ('said', 'mock'), ('said', 'caterpillar'), ('said', 'gryphon')]\n",
      "[('abide', 'figures'), ('acceptance', 'elegant'), ('accounting', 'tastes'), ('accustomed', 'usurpation'), ('act', 'crawling'), ('adjourn', 'immediate'), ('adoption', 'energetic'), ('affair', 'trusts'), ('agony', 'terror'), ('alarmed', 'proposal')] \n",
      "\n",
      "Trigram Association Measures:\n",
      "[('said', 'mock', 'turtle'), ('said', 'march', 'hare'), ('poor', 'little', 'thing'), ('little', 'golden', 'key'), ('march', 'hare', 'said'), ('mock', 'turtle', 'said'), ('white', 'kid', 'gloves'), ('beau', 'ootiful', 'soo'), ('certainly', 'said', 'alice'), ('might', 'well', 'say')]\n",
      "[('accustomed', 'usurpation', 'conquest'), ('adjourn', 'immediate', 'adoption'), ('adoption', 'energetic', 'remedies'), ('ancient', 'modern', 'seaography'), ('apple', 'roast', 'turkey'), ('arithmetic', 'ambition', 'distraction'), ('brother', 'latin', 'grammar'), ('canvas', 'bag', 'tied'), ('cherry', 'tart', 'custard'), ('circle', 'exact', 'shape')] \n",
      "\n",
      "Sentence tokenization:\n",
      "9\n",
      "['\\nElephants are large mammals of the family Elephantidae \\nand the order Proboscidea.', 'Two species are traditionally recognised, \\nthe African elephant and the Asian elephant.', 'Elephants are scattered \\nthroughout sub-Saharan Africa, South Asia, and Southeast Asia.'] \n",
      "\n",
      "Normalize text:\n",
      "Done strip\n",
      "Done char remove\n",
      "Done contract exp\n",
      "Done spec char remove\n",
      "0     Elephants are large mammals of the family Ele...\n",
      "1    Two species are traditionally recognised the A...\n",
      "2    Elephants are scattered throughout subSaharan ...\n",
      "dtype: object \n",
      "\n",
      "Chunks:\n",
      " [['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'], ['species', 'african elephant', 'asian elephant'], ['elephants', 'subsaharan africa south asia', 'southeast asia'], ['male african elephants', 'extant terrestrial animals'], ['elephants', 'long trunk', 'many purposes', 'water', 'grasping objects'], ['incisors', 'tusks', 'weapons', 'tools', 'objects'], ['elephants', 'flaps', 'body temperature'], ['pillarlike legs', 'great weight'], ['african elephants', 'ears', 'backs', 'asian elephants', 'ears', 'convex', 'level backs']] \n",
      "\n",
      "Top 30 TF-IDF keyphrases:]n [('extant terrestrial animals', 0.707), ('male african elephants', 0.707), ('great weight', 0.707), ('pillarlike legs', 0.707), ('southeast asia', 0.684), ('subsaharan africa south asia', 0.684), ('body temperature', 0.684), ('flaps', 0.684), ('ears', 0.667), ('african elephant', 0.577), ('asian elephant', 0.577), ('species', 0.577), ('family elephantidae', 0.565), ('large mammals', 0.565), ('order proboscidea', 0.565), ('grasping objects', 0.492), ('long trunk', 0.492), ('many purposes', 0.492), ('water', 0.492), ('incisors', 0.447), ('objects', 0.447), ('tools', 0.447), ('tusks', 0.447), ('weapons', 0.447), ('african elephants', 0.333), ('asian elephants', 0.333), ('backs', 0.333), ('convex', 0.333), ('level backs', 0.333), ('elephants', 0.253)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are several ways to get folders visible in Python. This way isn't the most elegant\n",
    "# but it works consistently. Replace my path with yours. The path you append to should be the\n",
    "# folder where your tokenizer Python class is located.\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\neugg\\OneDrive\\Documents\\GitHub\\dsc360-instructor\\12 Week\\week_4\\assignment')\n",
    "from text_normalizer import TextNormalizer\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "tn = TextNormalizer()\n",
    "\n",
    "# my code is a bit different than the author's but works with our\n",
    "# TextNormalizer.\n",
    "alice_txt = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice_list = list([' '.join(ts) for ts in alice_txt])\n",
    "alice = pd.Series(alice_list)\n",
    "norm_alice = tn.normalize_corpus(corpus=alice, text_lemmatization=False)\n",
    "\n",
    "# print first line\n",
    "print('\\nAlice - before and after')\n",
    "print(alice[0], '\\n', norm_alice[0], '\\n')\n",
    "\n",
    "# page 353\n",
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip()\n",
    "                     for document in corpus])\n",
    "\n",
    "# page 352\n",
    "def compute_ngrams(sequence, n):\n",
    "    return zip(*[sequence[index:]\n",
    "                 for index in range(n)])\n",
    "\n",
    "# page 353\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),\n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq)\n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams\n",
    "\n",
    "# page 353\n",
    "print('Bigrams:\\n', get_top_ngrams(corpus=norm_alice, ngram_val=2, limit=10), '\\n')\n",
    "\n",
    "# page 354\n",
    "print('Trigrams:\\n', get_top_ngrams(corpus=norm_alice, ngram_val=3, limit=10))\n",
    "\n",
    "# page 355\n",
    "print('Collocation Finder:\\n')\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures # updated package\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split()\n",
    "                                                for item\n",
    "                                                in norm_alice])\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "print('Bigram Association Measures:')\n",
    "print(finder.nbest(bigram_measures.raw_freq, 10))\n",
    "print(finder.nbest(bigram_measures.pmi, 10), '\\n')\n",
    "\n",
    "# page 356\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures # updated package\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split()\n",
    "                                                for item\n",
    "                                                in norm_alice])\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "print('Trigram Association Measures:')\n",
    "print(finder.nbest(trigram_measures.raw_freq, 10))\n",
    "print(finder.nbest(trigram_measures.pmi, 10), '\\n')\n",
    "\n",
    "# page 357\n",
    "sentences = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\"\n",
    "sent_tokens = nltk.sent_tokenize(sentences)\n",
    "print('Sentence tokenization:')\n",
    "print(len(sent_tokens))\n",
    "print(sent_tokens[:3], '\\n')\n",
    "\n",
    "print('Normalize text:')\n",
    "sentences_series = pd.Series(sent_tokens)\n",
    "norm_sentences = tn.normalize_corpus(corpus=sentences_series, text_lower_case=False,\n",
    "                                     text_lemmatization=False, stopword_removal=False)\n",
    "print(norm_sentences[:3], '\\n')\n",
    "\n",
    "# starting on page 358\n",
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def get_chunks(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "               stopword_list=stopwords):\n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tagged_sents = nltk.pos_tag_sents(\n",
    "            [nltk.word_tokenize(sentence)])\n",
    "\n",
    "        chunks = [chunker.parse(tagged_sent)\n",
    "                  for tagged_sent in tagged_sents]\n",
    "\n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                     for chunk in chunks]\n",
    "\n",
    "        flattened_chunks = list(itertools.chain.from_iterable(\n",
    "            wtc_sent for wtc_sent in wtc_sents)\n",
    "        )\n",
    "\n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk in\n",
    "                               itertools.groupby(flattened_chunks, lambda word_pos_chunk:\n",
    "                                word_pos_chunk[2] != 'O')]\n",
    "\n",
    "        valid_chunks = [' '.join(word.lower()\n",
    "                                 for word, tag, chunk in wtc_group\n",
    "                                    if word.lower() not in stopword_list)\n",
    "                                        for status, wtc_group in valid_chunks_tagged\n",
    "                                            if status]\n",
    "\n",
    "        all_chunks.append(valid_chunks)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "# page 360\n",
    "chunks = get_chunks(norm_sentences)\n",
    "print('Chunks:\\n', chunks, '\\n')\n",
    "\n",
    "# page 361\n",
    "from gensim import corpora, models\n",
    "\n",
    "def get_tfidf_weighted_keyphrases(sentences, grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "\n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "    weighted_phrases = {dictionary.get(idx): value for doc in corpus_tfidf for idx, value in doc}\n",
    "    weighted_phrases = sorted(weighted_phrases.items(), key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]\n",
    "\n",
    "    return weighted_phrases[:top_n]\n",
    "\n",
    "# top 30 tf-idf weighted keyphrases\n",
    "print('Top 30 TF-IDF keyphrases:]n', get_tfidf_weighted_keyphrases(sentences=norm_sentences, top_n=30), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(path, encoding, comment)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-02e9b468848d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# NOTE this code doesn't run in Python 3.7 - one of the sub-packages needs to be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# by the maintainers. You can switch to 3.6 or ship these 2 lines.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mkey_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Gensim\\'s summarization model results:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\summarization\\keywords.py\u001b[0m in \u001b[0;36mkeywords\u001b[1;34m(text, ratio, words, split, scores, pos_filter, lemmatize, deacc)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[1;31m# Gets a dict of word -> lemma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_clean_text_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m     \u001b[0msplit_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_tokenize_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\summarization\\textcleaner.py\u001b[0m in \u001b[0;36mclean_text_by_word\u001b[1;34m(text, deacc)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[0mfiltered_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjoin_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreprocess_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mHAS_PATTERN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# tag needs the context of the words in the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\en\\__init__.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(s, tokenize, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m    187\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\en\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(s, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \"\"\" Returns a tagged Unicode string.\n\u001b[0;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, s, tokenize, tags, chunks, relations, lemmata, encoding, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;31m# Tagger (required by chunker, labeler & lemmatizer).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mchunks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrelations\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlemmata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m                 \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1173\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\en\\__init__.py\u001b[0m in \u001b[0;36mfind_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tagset\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUNIVERSAL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpenntreebank2universal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mfind_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# [\"The\", \"cat\", \"purs\"] => [[\"The\", \"DT\"], [\"cat\", \"NN\"], [\"purs\", \"VB\"]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         return find_tags(tokens,\n\u001b[1;32m-> 1113\u001b[1;33m                     \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"lexicon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m                       \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                  \u001b[0mmorphology\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"morphology\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphology\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__len__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \"\"\"\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;31m# Arnold NNP x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;31m#--- FREQUENCY -------------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pattern\\text\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;31m# Arnold NNP x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         \u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;31m#--- FREQUENCY -------------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "# page 362\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "# NOTE this code doesn't run in Python 3.7 - one of the sub-packages needs to be updated\n",
    "# by the maintainers. You can switch to 3.6 or skip these 2 lines.\n",
    "key_words = keywords(sentences, ratio=1.0, scores=True, lemmatize=False)\n",
    "print('Gensim\\'s summarization model results:\\n', [(item, round(score, 3)) for item, score in key_words][:25])"
   ]
  },
  {
   "source": [
    "## Topic Modeling - Starting on Page 365"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['idx', 'MATLAB_NOTES', 'nips00', 'nips01', 'nips02', 'nips03', 'nips04', 'nips05', 'nips06', 'nips07', 'nips08', 'nips09', 'nips10', 'nips11', 'nips12', 'nips16', 'orig', 'RAW_DATA_NOTES', 'README_yann']\n",
      "Length of papers:\n",
      " 1740 \n",
      "\n",
      "Paper fragment:\n",
      " 1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a problem \n",
      "from examples using a local learning rule, we prove that the entropy of the \n",
      "problem becomes a lower bound for the connectivity of the network. \n",
      "INTRODUCTION \n",
      "The most distinguishing feature of neural networks is their ability to spon- \n",
      "taneously learn the desired function from 'training' samples, i.e., their ability \n",
      "to program themselves. Clearly, a given neural network cannot just learn any \n",
      "function, there must be some restrictions on which networks can learn which \n",
      "functions. One obv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# page 365\n",
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))\n",
    "\n",
    "# page 366\n",
    "folders = ['nips{0:02}'.format(i) for i in range(0, 13)]\n",
    "# Read all texts into a list.\n",
    "papers = []\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(DATA_PATH + folder)\n",
    "    for file_name in file_names:\n",
    "        with open(DATA_PATH + folder + '/' + file_name, encoding='utf-8',\n",
    "                  errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "# save the papers list, you'll need this a bit later on\n",
    "\n",
    "print('Length of papers:\\n', len(papers), '\\n')\n",
    "print('Paper fragment:\\n', papers[0][:1000], '\\n')"
   ]
  },
  {
   "source": [
    "## Text Wrangling - starting on page 367"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of normalized papers: 1740 \n\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens\n",
    "                        if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "    return norm_papers\n",
    "\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print('Length of normalized papers:', len(norm_papers), '\\n')"
   ]
  },
  {
   "source": [
    "## Text Representation with Feature Engineering - Starting on Page 369"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bigram model: \n",
      " ['connectivity', 'versus', 'entropy', 'yaser', 'abu_mostafa', 'california_institute', 'technology_pasadena', 'ca_abstract', 'doe', 'connectivity', 'neural_network', 'number', 'synapsis', 'per', 'neuron', 'relate', 'complexity', 'problem', 'handle', 'measured', 'entropy', 'switching', 'theory', 'would', 'suggest', 'relation', 'since', 'boolean_function', 'implemented', 'using', 'circuit', 'low', 'connectivity', 'using', 'two', 'input', 'nand', 'gate', 'however', 'network', 'learns', 'problem', 'example', 'using', 'local', 'learning', 'rule', 'prove', 'entropy', 'problem'] \n",
      "\n",
      "Sample word to number mappings:\n",
      " [(0, '0a'), (1, '2h'), (2, '2h2'), (3, '2he'), (4, '2n'), (5, '__c'), (6, '_c'), (7, '_k'), (8, 'a2'), (9, 'ability'), (10, 'abu_mostafa'), (11, 'access'), (12, 'accommodate'), (13, 'according'), (14, 'accumulated')] \n",
      "\n",
      "Total vocabulary size: 78892 \n",
      "\n",
      "Total vocabulary size: 7756 \n",
      "\n",
      "Bag of words:\n",
      " [(3, 1), (12, 3), (14, 1), (15, 1), (16, 1), (17, 16), (20, 1), (24, 1), (26, 1), (31, 3), (35, 1), (36, 1), (40, 3), (41, 5), (42, 1), (48, 1), (53, 3), (55, 1), (56, 2), (58, 1), (60, 3), (63, 5), (64, 4), (65, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 3), (82, 1), (83, 4), (84, 1), (85, 1), (86, 2), (94, 1), (96, 2), (97, 3), (106, 1), (110, 1), (119, 2), (120, 4), (121, 2), (124, 2), (127, 1), (128, 1), (132, 1), (133, 1), (135, 6), (136, 1), (144, 1)] \n",
      "\n",
      "Terms and counts:\n",
      " [('ability', 1), ('aip', 3), ('although', 1), ('american_institute', 1), ('amount', 1), ('analog', 16), ('appears', 1), ('architecture', 1), ('aspect', 1), ('available', 3), ('become', 1), ('becomes', 1), ('binary', 3), ('biological', 5), ('bit', 1), ('cannot', 1), ('circuit', 3), ('collective', 1), ('compare', 2), ('complex', 1), ('computing', 3), ('conference', 5), ('connected', 4), ('connectivity', 2), ('define', 1), ('defined', 1), ('defines', 1), ('definition', 1), ('denker', 3), ('designed', 1), ('desired', 4), ('diagonal', 1), ('difference', 1), ('directly', 2), ('ed', 1), ('el', 2), ('element', 3), ('equivalent', 1), ('eventually', 1), ('feature', 2), ('final', 4), ('find', 2), ('fixed', 2), ('frequency', 1), ('furthermore', 1), ('generating', 1), ('get', 1), ('global', 6), ('go', 1), ('hence', 1)] \n",
      "\n",
      "Total number of papers: 1740 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_')\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# sample demonstration\n",
    "print('Bigram model: \\n', bigram_model[norm_papers[0]][:50], '\\n')\n",
    "\n",
    "# page 370\n",
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings:\\n', list(dictionary.items())[:15], '\\n')\n",
    "print('Total vocabulary size:', len(dictionary), '\\n')\n",
    "\n",
    "# Filter out words that occur in fewer than 20 documents, or more than 50%\n",
    "# of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total vocabulary size:', len(dictionary), '\\n')\n",
    "\n",
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print('Bag of words:\\n', bow_corpus[1][:50], '\\n')\n",
    "\n",
    "# viewing actual terms and their counts\n",
    "print('Terms and counts:\\n', [(dictionary[idx], freq) for idx, freq in bow_corpus[1][:50]], '\\n')\n",
    "\n",
    "# total papers in the corpus\n",
    "print('Total number of papers:', len(bow_corpus), '\\n')"
   ]
  },
  {
   "source": [
    "## Latent Semantic Indexing - page 372\n",
    "This takes a while to run."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #1:\n",
      "0.215*\"unit\" + 0.212*\"state\" + 0.187*\"training\" + 0.177*\"neuron\" + 0.162*\"pattern\" + 0.145*\"image\" + 0.140*\"vector\" + 0.125*\"feature\" + 0.122*\"cell\" + 0.110*\"layer\" + 0.101*\"task\" + 0.097*\"class\" + 0.091*\"probability\" + 0.089*\"signal\" + 0.087*\"step\" + 0.086*\"response\" + 0.085*\"representation\" + 0.083*\"noise\" + 0.082*\"rule\" + 0.081*\"distribution\" \n",
      "\n",
      "Topic #2:\n",
      "0.487*\"neuron\" + 0.396*\"cell\" + -0.257*\"state\" + 0.191*\"response\" + -0.187*\"training\" + 0.170*\"stimulus\" + 0.117*\"activity\" + -0.109*\"class\" + 0.099*\"spike\" + 0.097*\"pattern\" + 0.096*\"circuit\" + 0.096*\"synaptic\" + -0.095*\"vector\" + 0.090*\"signal\" + 0.090*\"firing\" + 0.088*\"visual\" + -0.084*\"classifier\" + -0.083*\"action\" + -0.078*\"word\" + 0.078*\"cortical\" \n",
      "\n",
      "Topic #3:\n",
      "-0.627*\"state\" + 0.395*\"image\" + -0.219*\"neuron\" + 0.209*\"feature\" + -0.188*\"action\" + 0.137*\"unit\" + 0.131*\"object\" + -0.130*\"control\" + 0.129*\"training\" + -0.109*\"policy\" + 0.103*\"classifier\" + 0.090*\"class\" + -0.081*\"step\" + -0.081*\"dynamic\" + 0.080*\"classification\" + 0.078*\"layer\" + 0.076*\"recognition\" + -0.074*\"reinforcement_learning\" + 0.069*\"representation\" + 0.068*\"pattern\" \n",
      "\n",
      "Topic #4:\n",
      "0.686*\"unit\" + -0.433*\"image\" + 0.182*\"pattern\" + 0.131*\"layer\" + 0.123*\"hidden_unit\" + 0.121*\"net\" + 0.114*\"training\" + -0.112*\"feature\" + 0.109*\"activation\" + 0.107*\"rule\" + -0.097*\"neuron\" + 0.078*\"word\" + -0.070*\"pixel\" + 0.070*\"connection\" + -0.067*\"object\" + -0.065*\"state\" + -0.060*\"distribution\" + -0.059*\"face\" + 0.057*\"architecture\" + -0.055*\"estimate\" \n",
      "\n",
      "Topic #5:\n",
      "-0.428*\"image\" + -0.348*\"state\" + 0.266*\"neuron\" + -0.264*\"unit\" + 0.181*\"training\" + 0.174*\"class\" + -0.168*\"object\" + 0.167*\"classifier\" + -0.147*\"action\" + -0.122*\"visual\" + 0.117*\"vector\" + 0.115*\"node\" + 0.105*\"distribution\" + -0.103*\"motion\" + -0.099*\"feature\" + 0.097*\"classification\" + -0.097*\"control\" + -0.095*\"task\" + -0.087*\"cell\" + -0.083*\"representation\" \n",
      "\n",
      "Topic #6:\n",
      "-0.660*\"cell\" + 0.508*\"neuron\" + 0.213*\"image\" + 0.103*\"chip\" + 0.097*\"unit\" + -0.093*\"response\" + 0.090*\"object\" + -0.083*\"rat\" + -0.076*\"distribution\" + 0.070*\"circuit\" + -0.069*\"probability\" + -0.064*\"stimulus\" + 0.061*\"memory\" + 0.058*\"analog\" + 0.058*\"activation\" + -0.055*\"class\" + 0.053*\"bit\" + 0.052*\"net\" + -0.051*\"cortical\" + -0.050*\"firing\" \n",
      "\n",
      "Topic #7:\n",
      "-0.353*\"word\" + 0.281*\"unit\" + -0.272*\"training\" + -0.257*\"classifier\" + -0.177*\"recognition\" + 0.159*\"distribution\" + -0.152*\"feature\" + -0.144*\"state\" + -0.142*\"pattern\" + 0.141*\"vector\" + -0.128*\"cell\" + -0.128*\"task\" + 0.122*\"approximation\" + 0.121*\"variable\" + 0.110*\"equation\" + -0.107*\"classification\" + 0.106*\"noise\" + -0.103*\"class\" + 0.101*\"matrix\" + -0.098*\"neuron\" \n",
      "\n",
      "Topic #8:\n",
      "-0.303*\"pattern\" + 0.243*\"signal\" + 0.236*\"control\" + 0.202*\"training\" + -0.181*\"rule\" + -0.178*\"state\" + 0.167*\"noise\" + -0.166*\"class\" + 0.162*\"word\" + -0.155*\"cell\" + -0.154*\"feature\" + 0.147*\"motion\" + 0.140*\"task\" + -0.127*\"node\" + -0.124*\"neuron\" + 0.116*\"target\" + 0.114*\"circuit\" + -0.114*\"probability\" + -0.110*\"classifier\" + -0.109*\"image\" \n",
      "\n",
      "Topic #9:\n",
      "-0.472*\"node\" + -0.254*\"circuit\" + 0.214*\"word\" + -0.201*\"chip\" + 0.190*\"neuron\" + 0.172*\"stimulus\" + -0.160*\"classifier\" + -0.152*\"current\" + 0.147*\"feature\" + -0.146*\"voltage\" + 0.145*\"distribution\" + -0.141*\"control\" + -0.124*\"rule\" + -0.110*\"layer\" + -0.105*\"analog\" + -0.091*\"tree\" + 0.084*\"response\" + 0.080*\"state\" + 0.079*\"probability\" + 0.079*\"estimate\" \n",
      "\n",
      "Topic #10:\n",
      "0.518*\"word\" + -0.254*\"training\" + 0.236*\"vector\" + -0.222*\"task\" + -0.194*\"pattern\" + -0.156*\"classifier\" + 0.149*\"node\" + 0.146*\"recognition\" + -0.139*\"control\" + 0.138*\"sequence\" + -0.126*\"rule\" + 0.125*\"circuit\" + 0.123*\"cell\" + -0.113*\"action\" + -0.105*\"neuron\" + 0.094*\"hmm\" + 0.093*\"character\" + 0.088*\"chip\" + 0.088*\"matrix\" + 0.085*\"structure\" \n",
      "\n",
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.215), ('state', 0.212), ('training', 0.187), ('neuron', 0.177), ('pattern', 0.162), ('image', 0.145), ('vector', 0.14), ('feature', 0.125), ('cell', 0.122), ('layer', 0.11), ('task', 0.101), ('class', 0.097), ('probability', 0.091), ('signal', 0.089), ('step', 0.087), ('response', 0.086), ('representation', 0.085), ('noise', 0.083), ('rule', 0.082), ('distribution', 0.081)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.487), ('cell', 0.396), ('response', 0.191), ('stimulus', 0.17), ('activity', 0.117), ('spike', 0.099), ('pattern', 0.097), ('circuit', 0.096), ('synaptic', 0.096), ('signal', 0.09), ('firing', 0.09), ('visual', 0.088), ('cortical', 0.078)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.257), ('training', -0.187), ('class', -0.109), ('vector', -0.095), ('classifier', -0.084), ('action', -0.083), ('word', -0.078)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('image', 0.395), ('feature', 0.209), ('unit', 0.137), ('object', 0.131), ('training', 0.129), ('classifier', 0.103), ('class', 0.09), ('classification', 0.08), ('layer', 0.078), ('recognition', 0.076), ('representation', 0.069), ('pattern', 0.068)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -0.627), ('neuron', -0.219), ('action', -0.188), ('control', -0.13), ('policy', -0.109), ('step', -0.081), ('dynamic', -0.081), ('reinforcement_learning', -0.074)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.686), ('pattern', 0.182), ('layer', 0.131), ('hidden_unit', 0.123), ('net', 0.121), ('training', 0.114), ('activation', 0.109), ('rule', 0.107), ('word', 0.078), ('connection', 0.07), ('architecture', 0.057)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.433), ('feature', -0.112), ('neuron', -0.097), ('pixel', -0.07), ('object', -0.067), ('state', -0.065), ('distribution', -0.06), ('face', -0.059), ('estimate', -0.055)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.266), ('training', 0.181), ('class', 0.174), ('classifier', 0.167), ('vector', 0.117), ('node', 0.115), ('distribution', 0.105), ('classification', 0.097)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.428), ('state', -0.348), ('unit', -0.264), ('object', -0.168), ('action', -0.147), ('visual', -0.122), ('motion', -0.103), ('feature', -0.099), ('control', -0.097), ('task', -0.095), ('cell', -0.087), ('representation', -0.083)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 0.508), ('image', 0.213), ('chip', 0.103), ('unit', 0.097), ('object', 0.09), ('circuit', 0.07), ('memory', 0.061), ('analog', 0.058), ('activation', 0.058), ('bit', 0.053), ('net', 0.052)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -0.66), ('response', -0.093), ('rat', -0.083), ('distribution', -0.076), ('probability', -0.069), ('stimulus', -0.064), ('class', -0.055), ('cortical', -0.051), ('firing', -0.05)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('unit', 0.281), ('distribution', 0.159), ('vector', 0.141), ('approximation', 0.122), ('variable', 0.121), ('equation', 0.11), ('noise', 0.106), ('matrix', 0.101)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -0.353), ('training', -0.272), ('classifier', -0.257), ('recognition', -0.177), ('feature', -0.152), ('state', -0.144), ('pattern', -0.142), ('cell', -0.128), ('task', -0.128), ('classification', -0.107), ('class', -0.103), ('neuron', -0.098)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('signal', 0.243), ('control', 0.236), ('training', 0.202), ('noise', 0.167), ('word', 0.162), ('motion', 0.147), ('task', 0.14), ('target', 0.116), ('circuit', 0.114)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('pattern', -0.303), ('rule', -0.181), ('state', -0.178), ('class', -0.166), ('cell', -0.155), ('feature', -0.154), ('node', -0.127), ('neuron', -0.124), ('probability', -0.114), ('classifier', -0.11), ('image', -0.109)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.214), ('neuron', 0.19), ('stimulus', 0.172), ('feature', 0.147), ('distribution', 0.145), ('response', 0.084), ('state', 0.08), ('probability', 0.079), ('estimate', 0.079)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('node', -0.472), ('circuit', -0.254), ('chip', -0.201), ('classifier', -0.16), ('current', -0.152), ('voltage', -0.146), ('control', -0.141), ('rule', -0.124), ('layer', -0.11), ('analog', -0.105), ('tree', -0.091)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('word', 0.518), ('vector', 0.236), ('node', 0.149), ('recognition', 0.146), ('sequence', 0.138), ('circuit', 0.125), ('cell', 0.123), ('hmm', 0.094), ('character', 0.093), ('chip', 0.088), ('matrix', 0.088), ('structure', 0.085)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.254), ('task', -0.222), ('pattern', -0.194), ('classifier', -0.156), ('control', -0.139), ('rule', -0.126), ('action', -0.113), ('neuron', -0.105)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "(7756, 10) (10,) (10, 1740)\n",
      "      T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
      "0  0.016  0.017 -0.013 -0.008  0.024  0.028  0.000 -0.019  0.008 -0.006\n",
      "1  0.041  0.030 -0.019  0.021  0.019  0.056  0.018  0.009 -0.018 -0.011\n",
      "2  0.022 -0.000 -0.022 -0.008  0.011  0.016  0.013 -0.017  0.001  0.007\n",
      "3  0.032  0.036 -0.011  0.014  0.035  0.052 -0.016 -0.043  0.010 -0.029\n",
      "4  0.035 -0.002 -0.017  0.008  0.016  0.017  0.032 -0.022 -0.050  0.029\n",
      "Document #13:\n",
      "Dominant Topics (top 3): ['T3', 'T8', 'T9']\n",
      "Paper Summary:\n",
      "137 \n",
      "On the \n",
      "Power of Neural Networks for \n",
      "Solving Hard Problems \n",
      "Jehoshua Bruck \n",
      "Joseph W. Goodman \n",
      "Information Systems Laboratory \n",
      "Department of Electrical Engineering \n",
      "Stanford University \n",
      "Stanford, CA 94305 \n",
      "Abstract \n",
      "This paper deals with a neural network model in which each neuron \n",
      "performs a threshold logic function. An important property of the model \n",
      "is that it always converges to a stable state when operating in a serial \n",
      "mode [2,5]. This property is the basis of the potential applicat \n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T9', 'T8', 'T1']\n",
      "Paper Summary:\n",
      "542 Kassebaum, Tenorio and Schaefers \n",
      "The Cocktail Party Problem: \n",
      "Speech/Data Signal Separation Comparison \n",
      "between Backpropagation and SONN \n",
      "John Kassebaum \n",
      "jakec.ecn.purdue.edu \n",
      "Manoel Fernando Tenorio \n",
      "tenorioee.ecn.purdue.edu \n",
      "Chrlstoph Schaefers \n",
      "Parallel Distributed Structures Laboratory \n",
      "School of Electrical Engineering \n",
      "Purdue University \n",
      "W. Lafayette, IN. 47907 \n",
      "ABSTRACT \n",
      "This work introduces a new method called Self Organizing Neural \n",
      "Network (SONN) algorithm and compares its perfor \n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T1', 'T10', 'T7']\n",
      "Paper Summary:\n",
      "Learning Global Direct Inverse Kinematics \n",
      "David DeMers* \n",
      "Computer Science & Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0114 \n",
      "Kenneth Kreutz-Deigado I \n",
      "Electrical & Computer Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0407 \n",
      "Abstract \n",
      "We introduce and demonstrate a bootstrap method for construction of an in- \n",
      "verse function for the robot kinematic mapping using only sample configuration- \n",
      "space/workspace data. Unsupervised learning (clustering) techniques are used on \n",
      "pre-image neighborhoods in order to l \n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TOPICS = 10\n",
    "from gensim.models import LsiModel\n",
    "lsi_bow = LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
    "                   onepass=True, chunksize=1740, power_iters=1000)\n",
    "\n",
    "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #' + str(topic_id+1)+':')\n",
    "    print(topic, '\\n')\n",
    "\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #' + str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50, '\\n')\n",
    "\n",
    "# page 379\n",
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus],\n",
    "                                               len(singular_values)).T / singular_values).T\n",
    "print(term_topic.shape, singular_values.shape, topic_document.shape)\n",
    "\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3),\n",
    "                               columns=['T' + str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "print(document_topics.head(5))\n",
    "\n",
    "# page 380\n",
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(\n",
    "        -np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #' + str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500], '\\n')"
   ]
  },
  {
   "source": [
    "## Implementing LIS Topic Models from Scratch - starting on page 382"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7756, 1740)\n",
      "[[4. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] \n",
      "\n",
      "Total vocabulary size: 7756\n",
      "['2n' '_c' 'a2' ... 'support_vector' 'mozer_jordan' 'kearns_solla'] \n",
      "\n",
      "(7756, 10) (10,) (10, 1740) \n",
      "\n",
      "(10, 7756) \n",
      "\n",
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('word', 629.091), ('vector', 286.945), ('node', 181.498), ('recognition', 177.665), ('sequence', 168.05), ('circuit', 151.514), ('cell', 149.562), ('hmm', 113.762), ('character', 113.551), ('chip', 107.343), ('matrix', 107.113), ('structure', 103.444)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -309.125), ('task', -269.453), ('pattern', -235.696), ('classifier', -190.201), ('control', -169.136), ('rule', -153.281), ('action', -137.514), ('neuron', -127.48)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('node', 297.007), ('circuit', 159.406), ('chip', 126.143), ('classifier', 100.647), ('current', 95.72), ('voltage', 91.683), ('control', 88.633), ('rule', 77.639), ('layer', 69.017), ('analog', 65.723), ('tree', 57.392)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -134.299), ('neuron', -119.629), ('stimulus', -108.386), ('feature', -92.249), ('distribution', -91.05), ('response', -53.057), ('state', -50.296), ('probability', -49.879), ('estimate', -49.551)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('pattern', 176.005), ('rule', 105.002), ('state', 103.229), ('class', 96.689), ('cell', 90.249), ('feature', 89.689), ('node', 73.992), ('neuron', 72.222), ('probability', 65.924), ('classifier', 64.117), ('image', 63.289)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('signal', -141.147), ('control', -136.988), ('training', -117.183), ('noise', -96.897), ('word', -93.88), ('motion', -85.314), ('task', -81.076), ('target', -67.357), ('circuit', -66.401)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('word', 175.823), ('training', 135.256), ('classifier', 127.754), ('recognition', 87.973), ('feature', 75.489), ('state', 71.53), ('pattern', 70.858), ('cell', 63.967), ('task', 63.876), ('classification', 53.459), ('class', 51.347), ('neuron', 48.885)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -140.056), ('distribution', -79.374), ('vector', -70.049), ('approximation', -60.59), ('variable', -60.471), ('equation', -54.997), ('noise', -52.64), ('matrix', -50.22)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('cell', 323.293), ('response', 45.491), ('rat', 40.694), ('distribution', 37.425), ('probability', 33.697), ('stimulus', 31.434), ('class', 27.17), ('cortical', 25.095), ('firing', 24.501)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -248.989), ('image', -104.51), ('chip', -50.249), ('unit', -47.421), ('object', -44.117), ('circuit', -34.438), ('memory', -29.948), ('analog', -28.514), ('activation', -28.226), ('bit', -26.014), ('net', -25.677)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('image', 185.468), ('state', 150.47), ('unit', 114.133), ('object', 72.655), ('action', 63.771), ('visual', 52.603), ('motion', 44.737), ('feature', 43.022), ('control', 41.927), ('task', 41.104), ('cell', 37.454), ('representation', 35.86)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -114.972), ('training', -78.387), ('class', -75.332), ('classifier', -72.421), ('vector', -50.86), ('node', -49.808), ('distribution', -45.636), ('classification', -42.12)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('image', 181.442), ('feature', 46.775), ('neuron', 40.765), ('pixel', 29.499), ('object', 28.23), ('state', 27.354), ('distribution', 25.198), ('face', 24.591), ('estimate', 23.163)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -287.333), ('pattern', -76.299), ('layer', -54.921), ('hidden_unit', -51.376), ('net', -50.464), ('training', -47.694), ('activation', -45.616), ('rule', -44.867), ('word', -32.7), ('connection', -29.099), ('architecture', -23.905)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('state', 242.17), ('neuron', 84.418), ('action', 72.603), ('control', 50.09), ('policy', 41.938), ('step', 31.386), ('dynamic', 31.174), ('reinforcement_learning', 28.41)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -152.383), ('feature', -80.679), ('unit', -52.795), ('object', -50.645), ('training', -49.946), ('classifier', -39.791), ('class', -34.909), ('classification', -31.034), ('layer', -30.006), ('recognition', -29.37), ('representation', -26.702), ('pattern', -26.087)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('state', 94.2), ('training', 68.445), ('class', 40.099), ('vector', 34.747), ('classifier', 30.681), ('action', 30.403), ('word', 28.726)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -178.611), ('cell', -145.41), ('response', -69.867), ('stimulus', -62.286), ('activity', -42.88), ('spike', -36.194), ('pattern', -35.563), ('circuit', -35.356), ('synaptic', -35.169), ('signal', -33.059), ('firing', -33.019), ('visual', -32.42), ('cortical', -28.509)]\n",
      "-------------------------------------------------- \n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('unit', 78.138), ('state', 77.345), ('training', 68.106), ('neuron', 64.621), ('pattern', 59.094), ('image', 52.653), ('vector', 50.981), ('feature', 45.406), ('cell', 44.385), ('layer', 40.027), ('task', 36.67), ('class', 35.309), ('probability', 33.115), ('signal', 32.428), ('step', 31.52), ('response', 31.299), ('representation', 30.937), ('noise', 30.133), ('rule', 29.845), ('distribution', 29.654)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "-------------------------------------------------- \n",
      "\n",
      "Document #13:\n",
      "Dominant Topics (top 3): ['T8', 'T3', 'T2']\n",
      "Paper Summary:\n",
      "137 \n",
      "On the \n",
      "Power of Neural Networks for \n",
      "Solving Hard Problems \n",
      "Jehoshua Bruck \n",
      "Joseph W. Goodman \n",
      "Information Systems Laboratory \n",
      "Department of Electrical Engineering \n",
      "Stanford University \n",
      "Stanford, CA 94305 \n",
      "Abstract \n",
      "This paper deals with a neural network model in which each neuron \n",
      "performs a threshold logic function. An important property of the model \n",
      "is that it always converges to a stable state when operating in a serial \n",
      "mode [2,5]. This property is the basis of the potential applicat \n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T2', 'T3', 'T10']\n",
      "Paper Summary:\n",
      "542 Kassebaum, Tenorio and Schaefers \n",
      "The Cocktail Party Problem: \n",
      "Speech/Data Signal Separation Comparison \n",
      "between Backpropagation and SONN \n",
      "John Kassebaum \n",
      "jakec.ecn.purdue.edu \n",
      "Manoel Fernando Tenorio \n",
      "tenorioee.ecn.purdue.edu \n",
      "Chrlstoph Schaefers \n",
      "Parallel Distributed Structures Laboratory \n",
      "School of Electrical Engineering \n",
      "Purdue University \n",
      "W. Lafayette, IN. 47907 \n",
      "ABSTRACT \n",
      "This work introduces a new method called Self Organizing Neural \n",
      "Network (SONN) algorithm and compares its perfor \n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T10', 'T1', 'T4']\n",
      "Paper Summary:\n",
      "Learning Global Direct Inverse Kinematics \n",
      "David DeMers* \n",
      "Computer Science & Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0114 \n",
      "Kenneth Kreutz-Deigado I \n",
      "Electrical & Computer Eng. \n",
      "UC San Diego \n",
      "La Jolla, CA 92093-0407 \n",
      "Abstract \n",
      "We introduce and demonstrate a bootstrap method for construction of an in- \n",
      "verse function for the robot kinematic mapping using only sample configuration- \n",
      "space/workspace data. Unsupervised learning (clustering) techniques are used on \n",
      "pre-image neighborhoods in order to l \n",
      "\n"
     ]
    }
   ],
   "source": [
    "td_matrix = gensim.matutils.corpus2dense(corpus=bow_corpus,\n",
    "                                         num_terms=len(dictionary))\n",
    "print(td_matrix.shape)\n",
    "print(td_matrix, '\\n')\n",
    "\n",
    "vocabulary = np.array(list(dictionary.values()))\n",
    "print('Total vocabulary size:', len(vocabulary))\n",
    "print(vocabulary, '\\n')\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "u, s, vt = svds(td_matrix, k=TOTAL_TOPICS, maxiter=10000)\n",
    "term_topic = u\n",
    "topic_document = vt\n",
    "print(term_topic.shape, singular_values.shape, topic_document.shape, '\\n')\n",
    "\n",
    "tt_weights = term_topic.transpose() * singular_values[:, None]\n",
    "print(tt_weights.shape, '\\n')\n",
    "\n",
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(tt_weights), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([tt_weights[row, columns]\n",
    "                                  for row, columns in list(zip(np.arange(TOTAL_TOPICS),\n",
    "                                                               topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #' + str(n+1) + ':')\n",
    "    print('=' * 50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weights[n]\n",
    "    term_weights = sorted([(t, w)\n",
    "                           for t, w in zip(terms, weights)],\n",
    "                          key = lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-' * 50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-' * 50, '\\n')\n",
    "\n",
    "# page 387\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3),\n",
    "                               columns=['T' + str(i) for i in\n",
    "                                        range(1, TOTAL_TOPICS+1)])\n",
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(\n",
    "        -np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "\n",
    "    print('Document #' + str(document_number) + ':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500], '\\n')"
   ]
  },
  {
   "source": [
    "This next block is not required but shows how to save the embeddings and models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for use in the next file\n",
    "import pickle\n",
    "with open('data/papers.txt', 'wb') as fp:\n",
    "    pickle.dump(papers, fp)\n",
    "\n",
    "with open('data/bigram_model.txt', 'wb') as fp:\n",
    "    pickle.dump(bigram_model, fp)\n",
    "\n",
    "with open('data/norm_corpus_bigrams.txt', 'wb') as fp:\n",
    "    pickle.dump(norm_corpus_bigrams, fp)\n",
    "\n",
    "with open('data/bow_corpus.txt', 'wb') as fp:\n",
    "    pickle.dump(bow_corpus, fp)\n",
    "\n",
    "with open('data/dictionary.txt', 'wb') as fp:\n",
    "    pickle.dump(dictionary, fp)"
   ]
  },
  {
   "source": [
    "## Latent Dirichlet Allocation - Starting on Page 391\n",
    "This takes a while to run (and won't run by itself as a `.py` file unless you enclose it in a `main()` function)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LDA Topics with Weights:\n",
      "Topic #1:\n",
      "0.013*\"circuit\" + 0.012*\"chip\" + 0.008*\"neuron\" + 0.008*\"analog\" + 0.007*\"current\" + 0.007*\"bit\" + 0.006*\"voltage\" + 0.005*\"node\" + 0.005*\"word\" + 0.005*\"vector\" + 0.005*\"processor\" + 0.004*\"implementation\" + 0.004*\"threshold\" + 0.004*\"computation\" + 0.004*\"element\" + 0.004*\"signal\" + 0.004*\"pattern\" + 0.004*\"design\" + 0.004*\"memory\" + 0.004*\"parallel\" \n",
      "\n",
      "Topic #2:\n",
      "0.030*\"image\" + 0.012*\"object\" + 0.011*\"feature\" + 0.006*\"pixel\" + 0.006*\"visual\" + 0.005*\"representation\" + 0.005*\"recognition\" + 0.005*\"unit\" + 0.005*\"motion\" + 0.005*\"face\" + 0.005*\"task\" + 0.004*\"view\" + 0.004*\"layer\" + 0.004*\"human\" + 0.004*\"training\" + 0.004*\"position\" + 0.004*\"location\" + 0.004*\"region\" + 0.004*\"character\" + 0.003*\"vector\" \n",
      "\n",
      "Topic #3:\n",
      "0.020*\"neuron\" + 0.017*\"cell\" + 0.012*\"response\" + 0.010*\"stimulus\" + 0.007*\"spike\" + 0.007*\"signal\" + 0.006*\"activity\" + 0.006*\"synaptic\" + 0.005*\"firing\" + 0.005*\"frequency\" + 0.005*\"pattern\" + 0.004*\"current\" + 0.004*\"effect\" + 0.004*\"neural\" + 0.004*\"change\" + 0.004*\"et_al\" + 0.004*\"channel\" + 0.004*\"synapsis\" + 0.003*\"motion\" + 0.003*\"unit\" \n",
      "\n",
      "Topic #4:\n",
      "0.013*\"neuron\" + 0.009*\"cell\" + 0.009*\"pattern\" + 0.008*\"activity\" + 0.007*\"map\" + 0.006*\"unit\" + 0.006*\"dynamic\" + 0.006*\"visual\" + 0.005*\"layer\" + 0.005*\"receptive_field\" + 0.005*\"orientation\" + 0.005*\"connection\" + 0.005*\"cortical\" + 0.005*\"correlation\" + 0.004*\"response\" + 0.004*\"feature\" + 0.004*\"cortex\" + 0.004*\"stimulus\" + 0.004*\"phase\" + 0.004*\"neural\" \n",
      "\n",
      "Topic #5:\n",
      "0.020*\"unit\" + 0.011*\"state\" + 0.010*\"training\" + 0.008*\"rule\" + 0.007*\"net\" + 0.006*\"word\" + 0.006*\"pattern\" + 0.006*\"sequence\" + 0.006*\"node\" + 0.006*\"layer\" + 0.006*\"hidden_unit\" + 0.005*\"activation\" + 0.005*\"architecture\" + 0.005*\"recurrent\" + 0.004*\"recognition\" + 0.004*\"task\" + 0.004*\"vector\" + 0.004*\"trained\" + 0.004*\"context\" + 0.004*\"connection\" \n",
      "\n",
      "Topic #6:\n",
      "0.017*\"signal\" + 0.013*\"memory\" + 0.010*\"noise\" + 0.009*\"control\" + 0.008*\"trajectory\" + 0.007*\"dynamic\" + 0.007*\"state\" + 0.006*\"movement\" + 0.005*\"motor\" + 0.005*\"mapping\" + 0.004*\"pattern\" + 0.004*\"feedback\" + 0.004*\"capacity\" + 0.004*\"position\" + 0.004*\"speech\" + 0.004*\"training\" + 0.004*\"target\" + 0.004*\"arm\" + 0.004*\"vector\" + 0.004*\"change\" \n",
      "\n",
      "Topic #7:\n",
      "0.007*\"vector\" + 0.006*\"equation\" + 0.005*\"let\" + 0.005*\"linear\" + 0.005*\"distribution\" + 0.005*\"approximation\" + 0.005*\"matrix\" + 0.004*\"theorem\" + 0.004*\"convergence\" + 0.004*\"bound\" + 0.004*\"class\" + 0.004*\"training\" + 0.004*\"optimal\" + 0.004*\"theory\" + 0.004*\"consider\" + 0.004*\"solution\" + 0.004*\"probability\" + 0.004*\"estimate\" + 0.004*\"noise\" + 0.003*\"rate\" \n",
      "\n",
      "Topic #8:\n",
      "0.010*\"distribution\" + 0.009*\"probability\" + 0.009*\"variable\" + 0.008*\"mixture\" + 0.006*\"gaussian\" + 0.006*\"tree\" + 0.006*\"prior\" + 0.006*\"structure\" + 0.006*\"component\" + 0.006*\"node\" + 0.005*\"density\" + 0.005*\"class\" + 0.004*\"likelihood\" + 0.004*\"bayesian\" + 0.004*\"estimate\" + 0.004*\"sample\" + 0.004*\"step\" + 0.004*\"log\" + 0.004*\"source\" + 0.003*\"approximation\" \n",
      "\n",
      "Topic #9:\n",
      "0.017*\"training\" + 0.010*\"classifier\" + 0.008*\"classification\" + 0.008*\"class\" + 0.006*\"pattern\" + 0.006*\"feature\" + 0.006*\"test\" + 0.006*\"training_set\" + 0.005*\"vector\" + 0.005*\"prediction\" + 0.004*\"kernel\" + 0.004*\"experiment\" + 0.004*\"trained\" + 0.004*\"linear\" + 0.003*\"technique\" + 0.003*\"rbf\" + 0.003*\"task\" + 0.003*\"size\" + 0.003*\"table\" + 0.003*\"sample\" \n",
      "\n",
      "Topic #10:\n",
      "0.026*\"state\" + 0.013*\"action\" + 0.012*\"control\" + 0.008*\"policy\" + 0.007*\"task\" + 0.007*\"step\" + 0.006*\"reinforcement_learning\" + 0.006*\"controller\" + 0.006*\"environment\" + 0.005*\"optimal\" + 0.005*\"robot\" + 0.004*\"goal\" + 0.004*\"reward\" + 0.003*\"agent\" + 0.003*\"td\" + 0.003*\"current\" + 0.003*\"trial\" + 0.003*\"cost\" + 0.003*\"rate\" + 0.003*\"reinforcement\" \n",
      "\n",
      "Avg. Coherence Score: -0.9858031202745918 \n",
      "\n",
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('vector', 0.007), ('equation', 0.006), ('let', 0.005), ('linear', 0.005), ('distribution', 0.005), ('approximation', 0.005), ('matrix', 0.005), ('theorem', 0.004), ('convergence', 0.004), ('bound', 0.004), ('class', 0.004), ('training', 0.004), ('optimal', 0.004), ('theory', 0.004), ('consider', 0.004), ('solution', 0.004), ('probability', 0.004), ('estimate', 0.004), ('noise', 0.004), ('rate', 0.003)] \n",
      "\n",
      "Topic #2:\n",
      "[('training', 0.017), ('classifier', 0.01), ('classification', 0.008), ('class', 0.008), ('pattern', 0.006), ('feature', 0.006), ('test', 0.006), ('training_set', 0.006), ('vector', 0.005), ('prediction', 0.005), ('kernel', 0.004), ('experiment', 0.004), ('trained', 0.004), ('linear', 0.004), ('technique', 0.003), ('rbf', 0.003), ('task', 0.003), ('size', 0.003), ('table', 0.003), ('sample', 0.003)] \n",
      "\n",
      "Topic #3:\n",
      "[('neuron', 0.02), ('cell', 0.017), ('response', 0.012), ('stimulus', 0.01), ('spike', 0.007), ('signal', 0.007), ('activity', 0.006), ('synaptic', 0.006), ('firing', 0.005), ('frequency', 0.005), ('pattern', 0.005), ('current', 0.004), ('effect', 0.004), ('neural', 0.004), ('change', 0.004), ('et_al', 0.004), ('channel', 0.004), ('synapsis', 0.004), ('motion', 0.003), ('unit', 0.003)] \n",
      "\n",
      "Topic #4:\n",
      "[('unit', 0.02), ('state', 0.011), ('training', 0.01), ('rule', 0.008), ('net', 0.007), ('word', 0.006), ('pattern', 0.006), ('sequence', 0.006), ('node', 0.006), ('layer', 0.006), ('hidden_unit', 0.006), ('activation', 0.005), ('architecture', 0.005), ('recurrent', 0.005), ('recognition', 0.004), ('task', 0.004), ('vector', 0.004), ('trained', 0.004), ('context', 0.004), ('connection', 0.004)] \n",
      "\n",
      "Topic #5:\n",
      "[('circuit', 0.013), ('chip', 0.012), ('neuron', 0.008), ('analog', 0.008), ('current', 0.007), ('bit', 0.007), ('voltage', 0.006), ('node', 0.005), ('word', 0.005), ('vector', 0.005), ('processor', 0.005), ('implementation', 0.004), ('threshold', 0.004), ('computation', 0.004), ('element', 0.004), ('signal', 0.004), ('pattern', 0.004), ('design', 0.004), ('memory', 0.004), ('parallel', 0.004)] \n",
      "\n",
      "Topic #6:\n",
      "[('neuron', 0.013), ('cell', 0.009), ('pattern', 0.009), ('activity', 0.008), ('map', 0.007), ('unit', 0.006), ('dynamic', 0.006), ('visual', 0.006), ('layer', 0.005), ('receptive_field', 0.005), ('orientation', 0.005), ('connection', 0.005), ('cortical', 0.005), ('correlation', 0.005), ('response', 0.004), ('feature', 0.004), ('cortex', 0.004), ('stimulus', 0.004), ('phase', 0.004), ('neural', 0.004)] \n",
      "\n",
      "Topic #7:\n",
      "[('image', 0.03), ('object', 0.012), ('feature', 0.011), ('pixel', 0.006), ('visual', 0.006), ('representation', 0.005), ('recognition', 0.005), ('unit', 0.005), ('motion', 0.005), ('face', 0.005), ('task', 0.005), ('view', 0.004), ('layer', 0.004), ('human', 0.004), ('training', 0.004), ('position', 0.004), ('location', 0.004), ('region', 0.004), ('character', 0.004), ('vector', 0.003)] \n",
      "\n",
      "Topic #8:\n",
      "[('distribution', 0.01), ('probability', 0.009), ('variable', 0.009), ('mixture', 0.008), ('gaussian', 0.006), ('tree', 0.006), ('prior', 0.006), ('structure', 0.006), ('component', 0.006), ('node', 0.006), ('density', 0.005), ('class', 0.005), ('likelihood', 0.004), ('bayesian', 0.004), ('estimate', 0.004), ('sample', 0.004), ('step', 0.004), ('log', 0.004), ('source', 0.004), ('approximation', 0.003)] \n",
      "\n",
      "Topic #9:\n",
      "[('signal', 0.017), ('memory', 0.013), ('noise', 0.01), ('control', 0.009), ('trajectory', 0.008), ('dynamic', 0.007), ('state', 0.007), ('movement', 0.006), ('motor', 0.005), ('mapping', 0.005), ('pattern', 0.004), ('feedback', 0.004), ('capacity', 0.004), ('position', 0.004), ('speech', 0.004), ('training', 0.004), ('target', 0.004), ('arm', 0.004), ('vector', 0.004), ('change', 0.004)] \n",
      "\n",
      "Topic #10:\n",
      "[('state', 0.026), ('action', 0.013), ('control', 0.012), ('policy', 0.008), ('task', 0.007), ('step', 0.007), ('reinforcement_learning', 0.006), ('controller', 0.006), ('environment', 0.006), ('optimal', 0.005), ('robot', 0.005), ('goal', 0.004), ('reward', 0.004), ('agent', 0.003), ('td', 0.003), ('current', 0.003), ('trial', 0.003), ('cost', 0.003), ('rate', 0.003), ('reinforcement', 0.003)] \n",
      "\n",
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['vector', 'equation', 'let', 'linear', 'distribution', 'approximation', 'matrix', 'theorem', 'convergence', 'bound', 'class', 'training', 'optimal', 'theory', 'consider', 'solution', 'probability', 'estimate', 'noise', 'rate'] \n",
      "\n",
      "Topic #2:\n",
      "['training', 'classifier', 'classification', 'class', 'pattern', 'feature', 'test', 'training_set', 'vector', 'prediction', 'kernel', 'experiment', 'trained', 'linear', 'technique', 'rbf', 'task', 'size', 'table', 'sample'] \n",
      "\n",
      "Topic #3:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'spike', 'signal', 'activity', 'synaptic', 'firing', 'frequency', 'pattern', 'current', 'effect', 'neural', 'change', 'et_al', 'channel', 'synapsis', 'motion', 'unit'] \n",
      "\n",
      "Topic #4:\n",
      "['unit', 'state', 'training', 'rule', 'net', 'word', 'pattern', 'sequence', 'node', 'layer', 'hidden_unit', 'activation', 'architecture', 'recurrent', 'recognition', 'task', 'vector', 'trained', 'context', 'connection'] \n",
      "\n",
      "Topic #5:\n",
      "['circuit', 'chip', 'neuron', 'analog', 'current', 'bit', 'voltage', 'node', 'word', 'vector', 'processor', 'implementation', 'threshold', 'computation', 'element', 'signal', 'pattern', 'design', 'memory', 'parallel'] \n",
      "\n",
      "Topic #6:\n",
      "['neuron', 'cell', 'pattern', 'activity', 'map', 'unit', 'dynamic', 'visual', 'layer', 'receptive_field', 'orientation', 'connection', 'cortical', 'correlation', 'response', 'feature', 'cortex', 'stimulus', 'phase', 'neural'] \n",
      "\n",
      "Topic #7:\n",
      "['image', 'object', 'feature', 'pixel', 'visual', 'representation', 'recognition', 'unit', 'motion', 'face', 'task', 'view', 'layer', 'human', 'training', 'position', 'location', 'region', 'character', 'vector'] \n",
      "\n",
      "Topic #8:\n",
      "['distribution', 'probability', 'variable', 'mixture', 'gaussian', 'tree', 'prior', 'structure', 'component', 'node', 'density', 'class', 'likelihood', 'bayesian', 'estimate', 'sample', 'step', 'log', 'source', 'approximation'] \n",
      "\n",
      "Topic #9:\n",
      "['signal', 'memory', 'noise', 'control', 'trajectory', 'dynamic', 'state', 'movement', 'motor', 'mapping', 'pattern', 'feedback', 'capacity', 'position', 'speech', 'training', 'target', 'arm', 'vector', 'change'] \n",
      "\n",
      "Topic #10:\n",
      "['state', 'action', 'control', 'policy', 'task', 'step', 'reinforcement_learning', 'controller', 'environment', 'optimal', 'robot', 'goal', 'reward', 'agent', 'td', 'current', 'trial', 'cost', 'rate', 'reinforcement'] \n",
      "\n",
      "Avg. Coherence Score (cv): 0.4930044902277785\n",
      "Avg. Coherence Score (umass): -0.9858031202745918\n",
      "Model Perpelxity: -7.78786279030283 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOTAL_TOPICS = 10\n",
    "import gensim\n",
    "# page 391\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary,\n",
    "                                    chunksize=1740, alpha='auto', eta='auto',\n",
    "                                    random_state=42, iterations=500,\n",
    "                                    num_topics=TOTAL_TOPICS, passes=20,\n",
    "                                    eval_every=None)\n",
    "print('LDA Topics with Weights:')\n",
    "for topic_id, topic in lda_model.show_topics(num_topics=TOTAL_TOPICS, num_words=20):\n",
    "    print('Topic #' + str(topic_id+1) + ':')\n",
    "    print(topic, '\\n')\n",
    "\n",
    "# page 393\n",
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score, '\\n')\n",
    "\n",
    "# page 396\n",
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('=' * 50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #' + str(idx+1) + ':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic], '\\n')\n",
    "\n",
    "# page 397\n",
    "print('LDA Topics without Weights')\n",
    "print('=' * 50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #' + str(idx+1) + ':')\n",
    "    print([term for wt, term in topic], '\\n')\n",
    "\n",
    "# page 399\n",
    "cv_coherence_model_lda = \\\n",
    "    gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                    texts=norm_corpus_bigrams, dictionary=dictionary,\n",
    "                                    coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = \\\n",
    "    gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus,\n",
    "                                    texts=norm_corpus_bigrams, dictionary=dictionary,\n",
    "                                    coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (umass):', avg_coherence_umass)\n",
    "print('Model Perpelxity:', perplexity, '\\n')"
   ]
  },
  {
   "source": [
    "I skipped the LDA Models with MALLET since it was basically a repeat of the last section with minimal new information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Latent Dirichlet Allocation Tuning - Starting on Page 402\n",
    "WARNING: this file takes a very long time to run - well over 90 minutes. <BR>\n",
    "\n",
    "Note that this code will also not work in a `.py` file without being enclosed in a `main()` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|| 19/19 [1:53:17<00:00, 381.50s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7e918c98574c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                                                                 end_topic_count=20, step=1)\n\u001b[0;32m     27\u001b[0m coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1),\n\u001b[1;32m---> 28\u001b[1;33m                                 'Coherence Score': np.round(coherence_scores, 4)})\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoherence_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Coherence Score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "TOTAL_TOPICS = 10\n",
    "# NOTE the use of multicore to speed things up.\n",
    "from tqdm import tqdm\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, start_topic_count=2,\n",
    "                                    end_topic_count=10, step=1, workers=2):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count + 1, step)):\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary,\n",
    "                                               chunksize=1740,\n",
    "                                               random_state=42, iterations=500,\n",
    "                                               num_topics=topic_nums, passes=20,\n",
    "                                               eval_every=None, workers=workers)\n",
    "        cv_coherence_model_lda = \\\n",
    "            gensim.models.CoherenceModel(model=lda_model, corpus=corpus,\n",
    "                                         texts=texts, dictionary=dictionary,\n",
    "                                         coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(lda_model)\n",
    "    return models, coherence_scores\n",
    "\n",
    "# changed end_topic_count to 20 from 30 to speed things up\n",
    "lda_models, coherence_scores = topic_model_coherence_generator(bow_corpus, norm_corpus_bigrams,\n",
    "                                                                dictionary, start_topic_count=2,\n",
    "                                                                end_topic_count=30, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "range(2, 31)\n",
      "[<gensim.models.ldamulticore.LdaMulticore object at 0x0000025994E68EB8>, <gensim.models.ldamulticore.LdaMulticore object at 0x0000025986978A58>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259E75439E8>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259F4F0D5C0>, <gensim.models.ldamulticore.LdaMulticore object at 0x0000025995E44438>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259E7D0A0B8>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259E7D0A198>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259903E32B0>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259903E31D0>, <gensim.models.ldamulticore.LdaMulticore object at 0x0000025994D7AC50>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259952F07B8>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259E7D8C400>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259955B5E10>, <gensim.models.ldamulticore.LdaMulticore object at 0x0000025995106400>, <gensim.models.ldamulticore.LdaMulticore object at 0x0000025995C4B240>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259951499B0>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259989A2E80>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259962045C0>, <gensim.models.ldamulticore.LdaMulticore object at 0x00000259D7659B00>]\n",
      "<class 'list'>\n",
      "[0.3646 0.3965 0.4061 0.4082 0.4507 0.4727 0.4859 0.4934 0.493  0.491\n",
      " 0.5035 0.4981 0.4907 0.4806 0.4791 0.4859 0.4815 0.4783 0.4765]\n",
      "0.3646\n",
      "0.3965\n",
      "0.4061\n",
      "0.4082\n",
      "0.4507\n",
      "0.4727\n",
      "0.4859\n",
      "0.4934\n",
      "0.493\n",
      "0.491\n",
      "0.5035\n",
      "0.4981\n",
      "0.4907\n",
      "0.4806\n",
      "0.4791\n",
      "0.4859\n",
      "0.4815\n",
      "0.4783\n",
      "0.4765\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-8abaf23a3ec6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthis_score\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcoherence_scores\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcoherence_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Number of Topics'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Coherence Score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoherence_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoherence_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Coherence Score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "print(range(2, 31, 1))\n",
    "print(lda_models)\n",
    "print(type(coherence_scores))\n",
    "rounded_scores = np.round(coherence_scores, 4)\n",
    "print(rounded_scores)\n",
    "for this_score in coherence_scores:\n",
    "    print(np.round(this_score, 4))\n",
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1), 'Coherence Score': np.round(coherence_scores, 4)})\n",
    "print(coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(10))"
   ]
  },
  {
   "source": [
    "## Plot and Outputs - Starting on Page 404"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "x_ax = range(2, 31, 1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "y1 = plt.ylabel('Coherence Score')\n",
    "plt.show()\n",
    "\n",
    "# page 405\n",
    "best_model_idx = coherence_df[coherence_df['Number of Topics'] == 20].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "print('Number of topics for best LDA model:', best_lda_model.num_topics, '\\n')\n",
    "\n",
    "topics = [[(term, round(wt, 3)) for term, wt in best_lda_model.show_topic(n, topn=20)]\n",
    "            for n in range(0, best_lda_model.num_topics)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #' + str(idx+1) + ':')\n",
    "    print([term for term, wt in topic])\n",
    "print('\\n')\n",
    "\n",
    "# page 407\n",
    "topics_df = pd.DataFrame([[term for term in topic] for topic in topics],\n",
    "                            columns = ['Term' + str(i) for i in range(1, 21)],\n",
    "                            index=['Topic ' + str(t) for t in range(1, best_lda_model.num_topics+1)]).T\n",
    "print(topics_df, '\\n')\n",
    "\n",
    "# page 408\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])\n",
    "                            for topic in topics], columns = ['Terms per Topic'],\n",
    "                            index=['Topic' + str(t)\n",
    "                                for t in range(1, best_lda_model.num_topics+1)])\n",
    "print(topics_df, '\\n')"
   ]
  },
  {
   "source": [
    "## Interpreting model results - starting on page 409"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = best_lda_model[bow_corpus]\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0]\n",
    "                    for topics in tm_results]\n",
    "print('First five topics:', corpus_topics[:5],'\\n')\n",
    "\n",
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic']\n",
    "                                    for t in corpus_topics]\n",
    "corpus_topic_df['Paper'] = papers"
   ]
  },
  {
   "source": [
    "## Dominant Topics\n",
    "\n",
    "### Distribution Across Corpus - starting on page 410"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "topics_stats_df = corpus_topic_df.groupby('Dominant Topic').agg({'Dominant Topic': {\n",
    "    'Doc Count': np.size, '% Total Docs': np.size}})\n",
    "topics_stats_df = topics_stats_df['Dominant Topic'].reset_index()\n",
    "topics_stats_df['% Total Docs'] = topics_stats_df['% Total Docs'].apply(\n",
    "    lambda row: round((row*100) / len(papers), 2))\n",
    "topics_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic']\n",
    "                                    for t in range(len(topics_stats_df))]\n",
    "print('Topic Status DF:\\n', topics_stats_df)\n",
    "\n",
    "# Dominant Topics in Specific Research Papers - page 412\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "print(corpus_topic_df[corpus_topic_df['Document'].isin([681, 9, 392, 1622, 17, 906,\n",
    "                                                    996, 503, 13, 733])], '\\n')\n",
    "\n",
    "# Relevant Research Papers per Topic Based on Dominance - page 413\n",
    "print(corpus_topic_df.groupby('Dominant Topic').apply(\n",
    "    lambda topic_set: (topic_set.sort_values(by=['Contribution %'],\n",
    "                                                ascending=False).iloc[0])))"
   ]
  },
  {
   "source": [
    "## Predicting Topics - Starting on Page 415"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'HTMLParser'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-41cce7bd4945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;31m# only required if you start here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# This is the author's normalization file - best to use for this one script\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\neugg\\OneDrive\\Documents\\GitHub\\dsc360\\12 Week\\week_7\\normalization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mHTMLParser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'HTMLParser'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle # only required if you start here\n",
    "# This is the author's normalization file - best to use for this one script\n",
    "from normalization import normalize_corpus\\"
   ]
  },
  {
   "source": [
    "### Import Models (if you want to start here)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/best_lda_model.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-94b638330604>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/dictionary.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/best_lda_model.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mbest_lda_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/topics_df.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/best_lda_model.txt'"
     ]
    }
   ],
   "source": [
    "# Load from earlier work\n",
    "with open(\"data/bigram_model.txt\", \"rb\") as fp:\n",
    "    bigram_model = pickle.load(fp)\n",
    "with open(\"data/dictionary.txt\", \"rb\") as fp:\n",
    "    dictionary = pickle.load(fp)\n",
    "with open(\"data/best_lda_model.txt\", \"rb\") as fp:\n",
    "    best_lda_model = pickle.load(fp)\n",
    "with open(\"data/topics_df.txt\", \"rb\") as fp:\n",
    "    topics_df = pickle.load(fp)"
   ]
  },
  {
   "source": [
    "### Predicting Topics for New Research Papers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total New Papers: 4 \n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'normalize_corpus' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8fc5280afbe7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpaper_bow_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m norm_new_papers = text_preprocessing_pipeline(documents=new_papers, normalizer_fn=normalize_corpus,\n\u001b[0m\u001b[0;32m     23\u001b[0m                                               bigram_model=bigram_model)\n\u001b[0;32m     24\u001b[0m \u001b[0mnorm_bow_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_features_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_new_papers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "new_paper_files = glob.glob('nipstxt/nips16/nips16*.txt')\n",
    "new_papers = []\n",
    "\n",
    "for fn in new_paper_files:\n",
    "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "        new_papers.append(data)\n",
    "\n",
    "print('Total New Papers:', len(new_papers), '\\n')\n",
    "\n",
    "def text_preprocessing_pipeline(documents, normalizer_fn, bigram_model):\n",
    "    norm_docs = normalizer_fn(documents)\n",
    "    norm_docs_bigrams = bigram_model[norm_docs]\n",
    "    return norm_docs_bigrams\n",
    "\n",
    "def bow_features_pipeline(tokenized_docs, dictionary):\n",
    "    paper_bow_features = [dictionary.doc2bow(text) for text in\n",
    "                          tokenized_docs]\n",
    "    return paper_bow_features\n",
    "\n",
    "norm_new_papers = text_preprocessing_pipeline(documents=new_papers, normalizer_fn=normalize_corpus,\n",
    "                                              bigram_model=bigram_model)\n",
    "norm_bow_features = bow_features_pipeline(tokenized_docs=norm_new_papers, dictionary=dictionary)\n",
    "\n",
    "print(norm_new_papers[0][:30], '\\n')\n",
    "print(norm_bow_features[0][:30], '\\n')\n",
    "\n",
    "# page 416\n",
    "def get_topic_predictions(topic_model, corpus, topn=3):\n",
    "    topic_predictions = topic_model[corpus]\n",
    "    best_topics = [[(topic, round(wt, 3))\n",
    "                    for topic, wt in sorted(topic_predictions[i],\n",
    "                                            key=lambda row: -row[i])[:topn]]\n",
    "                    for i in range(len(topic_predictions))]\n",
    "    return best_topics\n",
    "\n",
    "# putting the function in action\n",
    "topic_preds = get_topic_predictions(topic_model=best_lda_model,\n",
    "                                    corpus=norm_bow_features, topn=2)\n",
    "print('Topic Predictions\\n', topic_preds)\n",
    "\n",
    "# page 417\n",
    "results_df = pd.DataFrame()\n",
    "results_df['Papers'] = range(1, len(new_papers)+1)\n",
    "results_df['Dominant Topics'] = [[top_num+1 for top_num, wt in item]\n",
    "                                 for item in topic_preds]\n",
    "res = results_df.set_index(['Papers'])['Dominant Topics']\\\n",
    "    .apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
    "results_df['Contribution %'] = [topic_wt for topic_list in\n",
    "                                [[round(wt*100, 2) for topic_num, wt in item]\n",
    "                                 for item in topic_preds]\n",
    "                                for topic_wt in topic_list]\n",
    "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic']\n",
    "                            for t in results_df['Dominant Topics'].values]\n",
    "results_df['Paper Desc'] = [new_papers[i-1][:200]\n",
    "                            for i in results_df.index_values]\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "print('Results for each paper\\n', results_df, '\\n')"
   ]
  },
  {
   "source": [
    "## Automated Document Summarization - starting on page 436"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "summarize() got an unexpected keyword argument 'ration'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a39e52e74c83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Summarized document:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOCUMENT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Limited document summary\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOCUMENT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: summarize() got an unexpected keyword argument 'ration'"
     ]
    }
   ],
   "source": [
    "# I took the description from:\n",
    "# Wikipedia. (2020). The Elder Scross V: Skyrim. https://en.wikipedia.org/wiki/The_Elder_Scrolls_V:_Skyrim\n",
    "DOCUMENT = \"\"\"\n",
    "The Elder Scrolls V: Skyrim is an action role-playing video game developed by Bethesda Game Studios and published by \n",
    "ethesda Softworks. It is the fifth main installment in The Elder Scrolls series, following The Elder Scrolls IV: \n",
    "Oblivion, and was released worldwide for Microsoft Windows, PlayStation 3, and Xbox 360 on November 11, 2011.\n",
    "\n",
    "The game's main story revolves around the player's character, the Dragonborn, on their quest to defeat Alduin the \n",
    "World-Eater, a dragon who is prophesied to destroy the world. The game is set 200 years after the events of Oblivion \n",
    "and takes place in Skyrim, the northernmost province of Tamriel. Over the course of the game, the player completes \n",
    "quests and develops the character by improving skills. The game continues the open-world tradition of its \n",
    "predecessors by allowing the player to travel anywhere in the game world at any time, and to ignore or postpone the \n",
    "main storyline indefinitely.\n",
    "\n",
    "Skyrim was developed using the Creation Engine, rebuilt specifically for the game. The team opted for a unique and \n",
    "more diverse open world than Oblivion's Imperial Province of Cyrodiil, which game director and executive producer \n",
    "Todd Howard considered less interesting by comparison. The game was released to critical acclaim, with reviewers \n",
    "particularly mentioning the character advancement and setting, and is considered to be one of the greatest video \n",
    "games of all time. Nonetheless it received some criticism, predominantly for its melee combat and numerous \n",
    "technical issues present at launch. The game shipped over seven million copies to retailers within the first week \n",
    "of its release, and over 30 million copies on all platforms as of November 2016, making it one of the best selling \n",
    "video games in history.\n",
    "\n",
    "Three downloadable content (DLC) add-ons were releasedDawnguard, Hearthfire, and Dragonbornwhich were repackaged \n",
    "into The Elder Scrolls V: Skyrim  Legendary Edition and released in June 2013. The Elder Scrolls V: Skyrim  \n",
    "Special Edition is a remastered version of the game released for Windows, Xbox One, and PlayStation 4 in October \n",
    "2016. It includes all three DLC expansions and a graphical upgrade, along with additional features such as modding \n",
    "capabilities on consoles. Versions were released in November 2017 for the Nintendo Switch and PlayStation VR, and \n",
    "a stand-alone virtual reality (VR) version for Windows was released in April 2018. These versions were based on \n",
    "the remastered release, but the Switch version's graphics upgrade was relative to its hardware capabilities, and \n",
    "it did not include the modding features.\n",
    "\"\"\"\n",
    "\n",
    "# page 438\n",
    "import re\n",
    "DOCUMENT = re.sub(r'\\n|\\r', ' ', DOCUMENT)\n",
    "DOCUMENT = re.sub(r' +', ' ', DOCUMENT)\n",
    "DOCUMENT = DOCUMENT.strip()\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "print('Summarized document:\\n', summarize(DOCUMENT, ration=0.2, split=False), '\\n')\n",
    "print('Limited document summary\\n', summarize(DOCUMENT, word_count=75, split=False), '\\n')\n",
    "\n",
    "# Text Wrangling - page 439\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}